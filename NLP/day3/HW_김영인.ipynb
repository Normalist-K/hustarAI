{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T05:22:34.570185Z",
     "start_time": "2020-10-21T05:22:28.814026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 846 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from nltk) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.10.15-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
      "\u001b[K     |████████████████████████████████| 662 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from nltk) (4.50.2)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=e916526263a7057a6891485d6a104085ffc9f6374ba5a070cb11ba205996b736\n",
      "  Stored in directory: /home/pirl/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.10.15\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:52.763913Z",
     "start_time": "2020-10-21T07:53:51.907289Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import statistics\n",
    "import nltk\n",
    "import random\n",
    "import collections\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:53.093411Z",
     "start_time": "2020-10-21T07:53:52.992275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Set cuda if available\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:53.148573Z",
     "start_time": "2020-10-21T07:53:53.136812Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dictionary class 선언\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, dataset, size):\n",
    "        ## init vocab ##\n",
    "        self.word2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "        self.idx2word = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "        self.build_dict(dataset, size)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>']) # if word does not exist in vocab then return unk idx\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def build_dict(self, dataset, dict_size):\n",
    "        \"\"\"Tokenize a text file.\"\"\"\n",
    "        total_words = (word for sent in dataset for word in sent) # store all words into tuple\n",
    "        word_freq = collections.Counter(total_words) # count the number of each word: ex) ('The': 10000, 'a': 5555, ...)\n",
    "        vocab = sorted(word_freq.keys(), key=lambda word: (-word_freq[word], word)) # sort by frequency\n",
    "        vocab = vocab[:dict_size]\n",
    "        for word in vocab:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:53.280167Z",
     "start_time": "2020-10-21T07:53:53.270598Z"
    }
   },
   "outputs": [],
   "source": [
    "## Brown dataset Preprocessing (NLTK)\n",
    "def brown_dataset(min_=5, max_=30):\n",
    "    nltk.download('brown')\n",
    "\n",
    "    # get sentences with the length between min and max\n",
    "    # convert all words into lower-case\n",
    "    all_seq = [[token.lower() for token in seq] for seq in nltk.corpus.brown.sents() \n",
    "               if min_ <= len(seq) <= max_]\n",
    "\n",
    "    random.shuffle(all_seq) # shuffle\n",
    "    return all_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:55.994074Z",
     "start_time": "2020-10-21T07:53:53.404194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/pirl/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'today', 'signed', 'an', 'executive', 'order', 'establishing', 'a', 'peace', 'corps', 'on', 'a', 'temporary', 'pilot', 'basis', '.']\n",
      "['if', 'the', 'doctor', 'is', 'conscientious', ',', 'he', 'wants', 'to', 'study', 'the', 'patient', '.']\n",
      "['he', 'who', 'had', 'tried', 'so', 'hard', ',', 'who', 'had', 'yearned', 'so', 'passionately', 'to', 'be', 'a', 'great', 'officer', '.']\n"
     ]
    }
   ],
   "source": [
    "## Download Brown dataset\n",
    "dataset = brown_dataset()\n",
    "\n",
    "## print some part\n",
    "print(dataset[0])\n",
    "print(dataset[1])\n",
    "print(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:56.003971Z",
     "start_time": "2020-10-21T07:53:55.995704Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data handler class 선언\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dataset, device, dict_size=20000, train_ratio=0.95):\n",
    "        train_size = int(len(dataset) * train_ratio)\n",
    "        valid_size = len(dataset) - train_size\n",
    "        self.device = device\n",
    "        self.dictionary = Dictionary(dataset, dict_size)\n",
    "        self.train = dataset[:train_size] # 0 ~ train_size\n",
    "        self.valid = dataset[train_size:train_size + valid_size] # train_size: train_size+valid_size\n",
    "\n",
    "    def indexing(self, dat):\n",
    "        src_idxes = [] # 모델 입력\n",
    "        tgt_idxes = [] # 모델 정답\n",
    "        for sent in dat:\n",
    "            src_idx = [self.dictionary('<sos>')] + [self.dictionary(word) for word in sent]\n",
    "            tgt_idx = [self.dictionary(word) for word in sent] + [self.dictionary('<eos>')]\n",
    "            src_idxes.append(torch.tensor(src_idx).type(torch.int64))\n",
    "            tgt_idxes.append(torch.tensor(tgt_idx).type(torch.int64))\n",
    "\n",
    "        src_idxes = rnn.pad_sequence(src_idxes, batch_first=True).to(self.device) # shape = [B, L]\n",
    "        tgt_idxes = rnn.pad_sequence(tgt_idxes, batch_first=True).to(self.device).view(-1) # flatten shape = [B * L]\n",
    "\n",
    "        return src_idxes, tgt_idxes\n",
    "\n",
    "    def batch_iter(self, batch_size, isTrain=True):\n",
    "        dat = self.train if isTrain else self.valid\n",
    "        if isTrain:\n",
    "            random.shuffle(dat)\n",
    "\n",
    "        for i in range(len(dat) // batch_size):\n",
    "            batch = dat[i * batch_size: (i+1) * batch_size]\n",
    "            src, tgt = self.indexing(batch)\n",
    "            yield {'src': src, 'tgt': tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:56.370340Z",
     "start_time": "2020-10-21T07:53:56.007364Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:56.383168Z",
     "start_time": "2020-10-21T07:53:56.372195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  <pad>      | index:     0 \n",
      "word:  <sos>      | index:     1 \n",
      "word:  <eos>      | index:     2 \n",
      "word:  <unk>      | index:     3 \n",
      "word:  the        | index:     4 \n",
      "word:  .          | index:     5 \n",
      "word:  ,          | index:     6 \n",
      "word:  of         | index:     7 \n",
      "word:  and        | index:     8 \n",
      "word:  to         | index:     9 \n",
      "word:  a          | index:    10 \n",
      "word:  in         | index:    11 \n",
      "word:  was        | index:    12 \n",
      "word:  he         | index:    13 \n",
      "word:  is         | index:    14 \n",
      "word:  ''         | index:    15 \n",
      "word:  ``         | index:    16 \n",
      "word:  it         | index:    17 \n",
      "word:  that       | index:    18 \n",
      "word:  for        | index:    19 \n",
      "word:  ;          | index:    20 \n"
     ]
    }
   ],
   "source": [
    "# Dictionary 확인\n",
    "\n",
    "for i, (key, val) in enumerate(corpus.dictionary.word2idx.items()):\n",
    "    print('word:  {:10s} | index: {:5d} '.format(key, val))\n",
    "    if i == 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:59.502846Z",
     "start_time": "2020-10-21T07:53:56.386485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['if', 'the', 'doctor', 'is', 'conscientious', ',', 'he', 'wants', 'to', 'study', 'the', 'patient', '.']]\n",
      "tensor([[   1,   63,    4,  898,   14, 8106,    6,   13, 1229,    9,  409,    4,\n",
      "         1266,    5]], device='cuda:0')\n",
      "tensor([  63,    4,  898,   14, 8106,    6,   13, 1229,    9,  409,    4, 1266,\n",
      "           5,    2], device='cuda:0')\n",
      "------------------------------------------------------------------------------------------\n",
      "[['i', 'have', 'today', 'signed', 'an', 'executive', 'order', 'establishing', 'a', 'peace', 'corps', 'on', 'a', 'temporary', 'pilot', 'basis', '.'], ['if', 'the', 'doctor', 'is', 'conscientious', ',', 'he', 'wants', 'to', 'study', 'the', 'patient', '.']]\n",
      "tensor([[   1,   27,   37,  326, 3058,   41, 2674,  304, 4166,   10,  510,  949,\n",
      "           24,   10, 3343, 2197,  646,    5],\n",
      "        [   1,   63,    4,  898,   14, 8106,    6,   13, 1229,    9,  409,    4,\n",
      "         1266,    5,    0,    0,    0,    0]], device='cuda:0')\n",
      "tensor([  27,   37,  326, 3058,   41, 2674,  304, 4166,   10,  510,  949,   24,\n",
      "          10, 3343, 2197,  646,    5,    2,   63,    4,  898,   14, 8106,    6,\n",
      "          13, 1229,    9,  409,    4, 1266,    5,    2,    0,    0,    0,    0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## indexing 함수 결과 확인\n",
    "\n",
    "# case : 단일 문장 입력 시. \n",
    "sent = [dataset[1]]\n",
    "idx_src, idx_tgt = corpus.indexing(sent)\n",
    "\n",
    "print(sent)\n",
    "print(idx_src) # <SOS> index로 시작\n",
    "print(idx_tgt) # <EOS> index로 종료\n",
    "\n",
    "print('-' * 90)\n",
    "## case : 복수 문장 입력 시 (batching)\n",
    "batch = [dataset[0], dataset[1]]\n",
    "idx_src, idx_tgt = corpus.indexing(batch)\n",
    "\n",
    "print(batch)\n",
    "print(idx_src) # 가장 길이가 긴 문장 (dataset[0]) 보다 짧은 문장 (dataset[1]) 의 경우 남는 길이만큼 padding=0 삽입 확인.\n",
    "print(idx_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:53:59.514337Z",
     "start_time": "2020-10-21T07:53:59.504213Z"
    }
   },
   "outputs": [],
   "source": [
    "## RNN Language model 선언\n",
    "\n",
    "# Define network\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, ntoken, hidden_size, nlayers, dropout=0.1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embeddings = nn.Embedding(ntoken, hidden_size, padding_idx=0) # word vector를 뽑아주는 함수, ntoken: 단어의 총 개수, hidden_size: hyperparameter(보통 2^n)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, nlayers, dropout=dropout, batch_first=True) # [B, L, Hidden_DIM], nlayers: 층 개수\n",
    "        self.output_layer = nn.Linear(hidden_size, ntoken)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.ntoken = ntoken\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_src, hidden):\n",
    "        emb = self.embeddings(input_src) # emb = (batch, length, dim)\n",
    "        output, hidden = self.rnn(emb, hidden) # output = (batch. length. dim), hidden : 가장 마지막 outstep의 결과\n",
    "        output = self.drop(output)\n",
    "        output = self.output_layer(output) # output = (batch, length, vocab_size)\n",
    "        output = output.view(-1, self.ntoken) # output = (batch * length, vocab_size)\n",
    "\n",
    "        return self.sm(output), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()) # to set init tensor with the same torch.dtype and torch.device\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.hidden_size),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:54:04.451926Z",
     "start_time": "2020-10-21T07:54:04.250281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 40\n",
    "hidden_size = 256 # 데이터 많을수록 크게\n",
    "dropout = 0.2\n",
    "max_epoch = 30\n",
    "nlayers = 2\n",
    "\n",
    "# build model\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, hidden_size, nlayers, dropout).to(device)\n",
    "isTrain = False # Flag variable\n",
    "\n",
    "# set loss func and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.NLLLoss(ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:54:04.456505Z",
     "start_time": "2020-10-21T07:54:04.453935Z"
    }
   },
   "outputs": [],
   "source": [
    "##### Training / Evaluation Parts #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:54:04.545936Z",
     "start_time": "2020-10-21T07:54:04.537288Z"
    }
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def cal_acc(scores, target):\n",
    "    pred = scores.max(-1)[1]\n",
    "    non_pad = target.ne(0)\n",
    "    num_correct = pred.eq(target).masked_select(non_pad).sum().item() \n",
    "    num_non_pad = non_pad.sum().item()\n",
    "    return 100 * (num_correct / num_non_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:54:04.716326Z",
     "start_time": "2020-10-21T07:54:04.703483Z"
    }
   },
   "outputs": [],
   "source": [
    "# train func.\n",
    "def train():\n",
    "    model.train() # Turn on training mode which enables dropout.\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in corpus.batch_iter(batch_size):\n",
    "        hidden = model.init_hidden(batch_size) # zero vectors for init hidden\n",
    "        target = batch['tgt'] # flattened target \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(batch['src'], hidden) # output = flatten output = [Batch_size * Length, vocab_size]\n",
    "\n",
    "        # output shape = (batch * length, vocab_size)\n",
    "        # target shape = (batch * length)   --> (batch * length, vocab_size) 로 one-hot distribtuion으로 내부적으로 변환되어 비교 수행\n",
    "        loss = criterion(output, target) # compare between vocab_prob and answer_prob(one-hot converted)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_acc.append(cal_acc(output, target))\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "\n",
    "    return mean_loss, total_time, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:54:04.995465Z",
     "start_time": "2020-10-21T07:54:04.984596Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation func.\n",
    "def evaluate():\n",
    "    model.eval() # Turn off dropout\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "\n",
    "    for batch in corpus.batch_iter(batch_size, isTrain=False):\n",
    "        with torch.no_grad():\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            target = batch['tgt']\n",
    "            output, hidden = model(batch['src'], hidden)\n",
    "            loss = criterion(output, target)\n",
    "            mean_loss.append(loss.item())\n",
    "            mean_acc.append(cal_acc(output, target))\n",
    "\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:54:05.706916Z",
     "start_time": "2020-10-21T07:54:05.695735Z"
    }
   },
   "outputs": [],
   "source": [
    "if isTrain: # set False if you don't need to train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        loss, epoch_time, accuracy = train()\n",
    "        print('epoch {:4d} | times {:3.3f} |  loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch+1, epoch_time, loss, accuracy))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            loss, accuracy = evaluate()\n",
    "            print('=' * 60)\n",
    "            print('Evaluation | loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch+1, epoch_time, loss, accuracy))\n",
    "            print('=' * 60)\n",
    "\n",
    "    with open('model.pt', 'wb') as f:\n",
    "        print('save model at: ./model.pt')\n",
    "        torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T08:10:06.007163Z",
     "start_time": "2020-10-21T08:10:05.995662Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred_sent_prob(sent):\n",
    "    import numpy as np\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. construct input for the model  : index 활용\n",
    "        # do someting about #1....\n",
    "        idx_src, _ = corpus.indexing(sent)\n",
    "        \n",
    "        # 2. set init hidden : 첫번째 step에 대한 zero vector\n",
    "        # do someting about #2....\n",
    "        hidden = model.init_hidden(1)\n",
    "        \n",
    "        # 3. get model output : vocab 크기의 log softmax 결과값 출력\n",
    "        # do someting about #3....\n",
    "        output, _ = model(idx_src, hidden)\n",
    "        # 4. get word log_prob : 찾고자 하는 단어의 log 확률값 추출\n",
    "        # do someting about #4....\n",
    "        log_probs = []\n",
    "        for i, word in enumerate(sent[0]):\n",
    "            idx = corpus.dictionary.word2idx[word]\n",
    "            log_probs.append(output[i][idx])\n",
    "        \n",
    "        # 5. comput sentence log_prob by summing each of word log_prob : 4의 각 결과를 합\n",
    "        # do someting about #5....\n",
    "        sent_prob = sum(log_probs)\n",
    "        \n",
    "        return sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T08:07:27.126771Z",
     "start_time": "2020-10-21T08:07:27.057398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from: ./model.pt\n",
      "tensor([[ -21.7967,  -21.5749,  -10.8941,  ...,  -18.1600,  -19.3086,\n",
      "          -20.0158],\n",
      "        [ -21.4231,  -21.3586,   -9.4515,  ...,  -15.7581,  -22.2106,\n",
      "          -23.0325],\n",
      "        [ -22.8331,  -22.5537,   -9.5518,  ...,  -25.8360,  -29.2622,\n",
      "          -29.5485],\n",
      "        [ -23.8862,  -23.8393,   -8.7553,  ...,  -55.0843,  -31.8204,\n",
      "          -41.7059],\n",
      "        [ -33.1614,  -33.7692,    0.0000,  ..., -159.4309, -138.7774,\n",
      "         -138.4866]], device='cuda:0')\n",
      "log prob of [the dog bark .]: -35.865\n",
      "tensor([[ -21.7967,  -21.5749,  -10.8941,  ...,  -18.1600,  -19.3086,\n",
      "          -20.0158],\n",
      "        [ -21.4231,  -21.3586,   -9.4515,  ...,  -15.7581,  -22.2106,\n",
      "          -23.0325],\n",
      "        [ -22.3386,  -22.0133,   -8.8538,  ...,  -30.9243,  -29.5918,\n",
      "          -30.7143],\n",
      "        [ -23.4326,  -23.4856,   -9.1123,  ...,  -36.2778,  -30.4442,\n",
      "          -30.8298],\n",
      "        [ -32.4423,  -33.2630,    0.0000,  ..., -164.3550, -144.7890,\n",
      "         -135.6121]], device='cuda:0')\n",
      "log prob of [the cat bark .]: -46.364\n",
      "tensor([[ -21.7967,  -21.5749,  -10.8941,  ...,  -18.1600,  -19.3086,\n",
      "          -20.0158],\n",
      "        [ -21.4792,  -21.4619,   -9.9177,  ...,  -38.9303,  -26.1195,\n",
      "          -34.7797],\n",
      "        [ -25.0253,  -25.5426,  -13.5963,  ...,  -27.1321,  -31.5807,\n",
      "          -27.5580],\n",
      "        [ -23.6024,  -23.8995,  -10.4190,  ...,  -24.3538,  -30.7609,\n",
      "          -24.7993],\n",
      "        [ -19.9796,  -20.1241,   -9.5269,  ...,  -31.2377,  -37.7855,\n",
      "          -26.3229],\n",
      "        [ -28.6283,  -28.4967,    0.0000,  ..., -125.1257, -113.3048,\n",
      "         -101.7094]], device='cuda:0')\n",
      "log prob of [boy am a i .]: -42.253\n",
      "tensor([[ -21.7967,  -21.5749,  -10.8941,  ...,  -18.1600,  -19.3086,\n",
      "          -20.0158],\n",
      "        [ -27.8956,  -27.4782,  -14.3106,  ...,  -37.9737,  -34.1929,\n",
      "          -34.7622],\n",
      "        [ -25.2141,  -25.4782,  -13.3014,  ...,  -35.8983,  -47.1165,\n",
      "          -31.2959],\n",
      "        [ -23.8074,  -24.2922,  -10.4659,  ...,  -26.3562,  -35.3620,\n",
      "          -28.6626],\n",
      "        [ -18.8008,  -18.7571,   -6.9850,  ...,  -33.7587,  -36.6765,\n",
      "          -28.6844],\n",
      "        [ -38.1107,  -39.0769,    0.0000,  ..., -166.7004, -142.1434,\n",
      "         -137.3307]], device='cuda:0')\n",
      "log prob of [i am a boy .]: -18.611\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "with open('./model.pt', 'rb') as f:\n",
    "    print('load model from: ./model.pt')\n",
    "    model = torch.load(f).to(device)\n",
    "\n",
    "    print('log prob of [the dog bark .]: {:3.3f}'.format(pred_sent_prob([['the', 'dog', 'bark', '.']])))\n",
    "    print('log prob of [the cat bark .]: {:3.3f}'.format(pred_sent_prob([['the', 'cat', 'bark', '.']])))\n",
    "\n",
    "    print('log prob of [boy am a i .]: {:3.3f}'.format(pred_sent_prob([['boy', 'am', 'a', 'i', '.']])))\n",
    "    print('log prob of [i am a boy .]: {:3.3f}'.format(pred_sent_prob([['i', 'am', 'a', 'boy', '.']])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
