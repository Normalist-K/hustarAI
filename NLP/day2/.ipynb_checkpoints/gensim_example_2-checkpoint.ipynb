{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Prepare the training text\n",
    "# Text Iterator for training\n",
    "class TextIterator(object):\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "        for line in open(self.fname):\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'newskor.txt'\n",
    "sentences = TextIterator(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2. Train Word2Vec Model\n",
    "model = gensim.models.Word2Vec(size=150, window=5, min_count=5, workers=5)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_words=len(model.wv.vocab), epochs=model.epochs)\n",
    "# save the trained model\n",
    "model.save('newskor.model')\n",
    "# save the model in word2vec format\n",
    "model.wv.save_word2vec_format('./newskor_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Load trained model\n",
    "model = gensim.models.Word2Vec.load('newskor.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Printing existing vocabularies\n",
    "print(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Get word similarity\n",
    "#word1 = '한국'\n",
    "#word2 = '북한'\n",
    "print (\"Caculate the similarity between word 1 and word 2\")\n",
    "word1 = input(\"word1: \")\n",
    "word2 = input(\"word2: \")\n",
    "\n",
    "# check the words are in the vocabulary\n",
    "no_problem = True\n",
    "vocab = model.wv.vocab\n",
    "\n",
    "if word1 not in vocab:\n",
    "    print ('the word ' + word1 + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "if word2 not in vocab:\n",
    "    print ('the word ' + word2 + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "if no_problem:\n",
    "    similarity = model.wv.similarity(word1, word2)\n",
    "    print ('the similarity between ' + word1 + ' and ' + word2 + ' : ', similarity)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Find mismatch word\n",
    "#words = '소프트웨어 네트워크 프로그램 데이터'\n",
    "print(\"Find one mismatching word among given words\")\n",
    "text = input(\"Words: \")\n",
    "words = text.split()\n",
    "\n",
    "# check the words are in the vocabulary\n",
    "no_problem = True\n",
    "vocab = model.wv.vocab\n",
    "\n",
    "for word in words:\n",
    "    if word not in vocab:\n",
    "        print('The word ' + word + ' is not in the vocabulary')\n",
    "        no_problem = False\n",
    "        break;\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "if no_problem:\n",
    "    mismatch = model.wv.doesnt_match(words)\n",
    "    print ('The mismatching word among ' + text +' is', mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Find the top-N most similar words\n",
    "print(\"Print the most similar words to a given word\")\n",
    "word = input(\"Word: \")\n",
    "\n",
    "no_problem = True\n",
    "vocab = model.wv.vocab\n",
    "\n",
    "if word not in vocab:\n",
    "\tprint ('The word ' + word + ' is not in the vocabulary')\n",
    "\tno_problem = False\n",
    "\n",
    "if no_problem:\n",
    "    print(model.wv.most_similar(positive=[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.5. Find the top-N most similar words with combination of words\n",
    "#word_a = '한국'\n",
    "#word_b = '아시아'\n",
    "#word_c = '유럽'\n",
    "print('Find the result of ( a - b + c )')\n",
    "word_a = input(\"a: \")\n",
    "word_b = input(\"b: \")\n",
    "word_c = input(\"c: \")\n",
    "\n",
    "# check the words are in the vocabulary\n",
    "no_problem = True\n",
    "vocab = model.wv.vocab\n",
    "\n",
    "if word_a not in vocab:\n",
    "\tprint ('the word ' + word_a + ' is not in the vocabulary')\n",
    "\tno_problem = False\n",
    "\n",
    "if word_b not in vocab:\n",
    "\tprint ('the word ' + word_b + ' is not in the vocabulary')\n",
    "\tno_problem = False\n",
    "\n",
    "if word_c not in vocab:\n",
    "\tprint ('the word ' + word_c + ' is not in the vocabulary')\n",
    "\tno_problem = False\n",
    "\n",
    "if no_problem:\n",
    "\tmostsimilar = model.wv.most_similar(positive=[word_a, word_c], negative=[word_b], topn=5)\n",
    "\tprint ('The result is ' + word_a + ' - ' + word_b + ' + ' + word_c + ' is', mostsimilar[0][0], mostsimilar[1][0], mostsimilar[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
