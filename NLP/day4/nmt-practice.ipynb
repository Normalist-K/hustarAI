{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T04:53:12.276112Z",
     "start_time": "2020-10-22T04:53:08.171331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchtext) (4.50.2)\n",
      "Requirement already satisfied: torch in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchtext) (1.6.0)\n",
      "Requirement already satisfied: numpy in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchtext) (1.19.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchtext) (2.24.0)\n",
      "Requirement already satisfied: future in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torch->torchtext) (0.18.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchtext) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchtext) (2020.6.20)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T04:53:28.652415Z",
     "start_time": "2020-10-22T04:53:22.712286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mosestokenizer\n",
      "  Downloading mosestokenizer-1.1.0.tar.gz (37 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting openfile\n",
      "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
      "Collecting uctools\n",
      "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
      "Collecting toolwrapper\n",
      "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: mosestokenizer, docopt, uctools, toolwrapper\n",
      "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-py3-none-any.whl size=49118 sha256=a896b8841ff5f6bb663e3cdfe8898902a975210a7a6682d26f54b6b40f759f29\n",
      "  Stored in directory: /home/pirl/.cache/pip/wheels/a7/31/94/fef279382208e85a65c1a7f5c4d0020115477b0af74f296b57\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=ae99abdef58192747a24934f81dcb0a13a2c00a4849e87d99b037b35db6d6d5a\n",
      "  Stored in directory: /home/pirl/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
      "  Building wheel for uctools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6162 sha256=dab5d152a45089ec9d253a596be09542c66af56844267cd633c4cb541a573aa5\n",
      "  Stored in directory: /home/pirl/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n",
      "  Building wheel for toolwrapper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3354 sha256=8827c19ad67bc05d41cabd6a378e9f297a50608370a68fd1a1ed7483dedf18f8\n",
      "  Stored in directory: /home/pirl/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n",
      "Successfully built mosestokenizer docopt uctools toolwrapper\n",
      "Installing collected packages: docopt, openfile, uctools, toolwrapper, mosestokenizer\n",
      "Successfully installed docopt-0.6.2 mosestokenizer-1.1.0 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mosestokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T04:53:37.960301Z",
     "start_time": "2020-10-22T04:53:36.398396Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Example\n",
    "from mosestokenizer import *\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Author: WonKee Lee (POSTECH)\n",
    "# \"Neural Machine Translation by Jointly Learning to Align and Translate\" 논문의 model 재현 (Toy code)\n",
    "#  (https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html 를 참고하여 수정함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### torchtext #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:27:19.097552Z",
     "start_time": "2020-10-22T05:27:19.063820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BOS = '<s>'    # Start symbol\n",
    "EOS = '</s>'   # End symbol\n",
    "PAD = '<pad>'  # padding symbol\n",
    "\n",
    "# ex) 'I am a boy.' -> ['I', 'am', 'a', 'boy']\n",
    "tok_en = MosesTokenizer('en')\n",
    "tok_fr = MosesTokenizer('fr')\n",
    "\n",
    "# Field: Tensor로 표현할 데이터의 타입, 처리 프로세스 등을 정의하는 객체\n",
    "src = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_en,\n",
    "            lower=True,\n",
    "            batch_first=True) # if=True shape:[Batch, length] else shape=[length, Batch]\n",
    "\n",
    "tgt = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_fr,\n",
    "            lower=True,\n",
    "            init_token=BOS,\n",
    "            eos_token=EOS,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:29:26.329049Z",
     "start_time": "2020-10-22T05:28:44.470103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "prefix_f = 'data/data'\n",
    "\n",
    "# parallel data 각각 (en, de) 을 src Field 와 tgt Field에 정의된 형태로 처리.\n",
    "parallel_dataset = TranslationDataset(path=prefix_f, exts=('.en', '.fr'), #exts: 확장자\n",
    "                                      fields=[('src', src), ('tgt', tgt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:29:26.341002Z",
     "start_time": "2020-10-22T05:29:26.333104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.translation.TranslationDataset object at 0x7fc08c77b610>\n",
      "dict_items([('src', ['you', 'were', 'in', 'a', 'coma', '.']), ('tgt', ['tu', 'étais', 'dans', 'le', 'coma', '.'])])\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset) \n",
    "\n",
    "print(parallel_dataset.examples[22222].__dict__.items()) # src 및 tgt 에 대한 samples 를 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:30:14.496779Z",
     "start_time": "2020-10-22T05:30:14.489539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'were', 'in', 'a', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].src) # src 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:30:14.846394Z",
     "start_time": "2020-10-22T05:30:14.838936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tu', 'étais', 'dans', 'le', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].tgt) # tgt 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:31:04.777722Z",
     "start_time": "2020-10-22T05:31:04.029299Z"
    }
   },
   "outputs": [],
   "source": [
    "##### 사전 구축 ########\n",
    "# src, tgt 필드에 사전 구축\n",
    "src.build_vocab(parallel_dataset, max_size=15000) # src 데이터의 frequency를 기반으로 dict를 만들어 줌\n",
    "tgt.build_vocab(parallel_dataset, max_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:31:10.854504Z",
     "start_time": "2020-10-22T05:31:10.837296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
      "\n",
      "     <unk> |   0\n",
      "     <pad> |   1\n",
      "         . |   2\n",
      "         i |   3\n",
      "       you |   4\n",
      "        to |   5\n",
      "       the |   6\n",
      "         ? |   7\n",
      "         a |   8\n",
      "   &apos;t |   9\n",
      "        is |  10\n",
      "        it |  11\n",
      "        he |  12\n",
      "      that |  13\n",
      "   &apos;s |  14\n",
      "        of |  15\n"
     ]
    }
   ],
   "source": [
    "# 사전 내용 \n",
    "print(src.vocab.__dict__.keys())\n",
    "print('')\n",
    "# stoi : string to index 의 약자\n",
    "for i, (k, v) in enumerate(src.vocab.stoi.items()):\n",
    "    print ('{:>10s} | {:>3d}'.format(k, v))\n",
    "    if i == 15 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:32:29.077825Z",
     "start_time": "2020-10-22T05:32:28.911430Z"
    }
   },
   "outputs": [],
   "source": [
    "train, valid = parallel_dataset.split(split_ratio=0.95) # 0.95 = train / 0.05 = valid 데이터로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:34:25.253902Z",
     "start_time": "2020-10-22T05:34:25.144343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Batch iterator 생성.\n",
    "# iterator 를 반복하며 batch (src, tgt) 가 생성 됨.\n",
    "BATCH_SIZE = 3\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:34:36.534986Z",
     "start_time": "2020-10-22T05:34:31.314556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirl/anaconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# iterator 는 Batch 객체 (Tensor) 를 출력해주며, \n",
    "# Batch.src / Batch.tgt 로 parallel data각각에 대해 접근가능.\n",
    "\n",
    "# 예시.\n",
    "Batch = next(iter(train_iterator)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:34:36.554384Z",
     "start_time": "2020-10-22T05:34:36.536841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  25,  148,   13,  247,  360,    7],\n",
       "        [  12,  854, 1429,   15,    4,    2],\n",
       "        [  25,   10,    6,  154,   53,    7]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src 에 저장된 데이터 출력\n",
    "# Field에 정의된 형식으로 데이터 전처리 (indexing 포함.)\n",
    "# 가장 긴 문장을 기준으로, 그 보다 짧은 문장은 Padding idx(=1) 을 부여.\n",
    "Batch.src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:34:41.334986Z",
     "start_time": "2020-10-22T05:34:41.322221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    37,    11,     7,    22,    13,    45,   235,   116,    99,\n",
       "             8,     3],\n",
       "        [    2,    12,    19,    75, 11771,    14,    92,   288,     4,     3,\n",
       "             1,     1],\n",
       "        [    2,     6,   115,   247,    22,   170,     8,     3,     1,     1,\n",
       "             1,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field에 정의된 형식으로 데이터 전처리 (indexing + bos + eos + pad 토큰 처리 됨.)\n",
    "Batch.tgt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:37:24.784591Z",
     "start_time": "2020-10-22T05:37:24.751653Z"
    }
   },
   "outputs": [],
   "source": [
    "### Encoder 정의.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, src_ntoken: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.src_ntoken = src_ntoken # 사전의 개수\n",
    "\n",
    "        self.embedding = nn.Embedding(src_ntoken, hidden_dim, \n",
    "                                      padding_idx=src.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, # input dim\n",
    "                          hidden_dim, # output dim\n",
    "                          bidirectional = True, \n",
    "                          batch_first=True) # batch_first = [B, L, dim]\n",
    "        \n",
    "        \n",
    "        # bidirectional hidden을 하나의 hidden size로 mapping해주기 위한 Linear\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = (Batch, Length) Tensor\n",
    "        embedded = self.dropout(self.embedding(src)) # shape = (Batch, Length, hidden_dim)\n",
    "\n",
    "        # outputs: [B, L, D*2], hidden: [2, B, D] -> [1, B, D] + [1, B, D]\n",
    "        # Note: Bidirection=False 인 경우:  outputs: [B, L, D], hidden: [1, B, D]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        last_hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # bidirection Dim(x2)을 projection --> [B, D]\n",
    "        hidden = torch.tanh(last_hidden).unsqueeze(0) # last bidirectional hidden (=Decoder init hidden) --> [1, B, D]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:37:25.030396Z",
     "start_time": "2020-10-22T05:37:25.014835Z"
    }
   },
   "outputs": [],
   "source": [
    "### Attention 모듈 정의 ###\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        attn_in = (enc_hid_dim * 2) + dec_hid_dim # bidirectional hidden + dec_hidden\n",
    "        self.linear = nn.Linear(attn_in, attn_dim)\n",
    "        self.merge = nn.Linear(attn_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hiden = (Batch, 1, Dim) 길이가 1씩 들어오기 때문.\n",
    "        src_len = encoder_outputs.shape[1] \n",
    "        repeated_decoder_hidden = decoder_hidden.repeat(1, src_len, 1) # [B, src_len, D] -> 각각의 src단어와 연산해주기 위해 늘려준 결과.\n",
    "\n",
    "        # enc의 각 step의 hidden + decoder의 hidden 의 결과값 # [B, src_len, D*2] --> [B, src_len, D]\n",
    "        # tanh(W*h_dec  + U*h_enc) 수식 부분.\n",
    "        energy = torch.tanh(self.linear(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2))) \n",
    "\n",
    "        score = self.merge(energy).squeeze(-1) # [B, src_len] 각 src 단어에 대한 점수 -> V^T tanh(W*h_dec  + U*h_enc) 부분\n",
    "        normalized_score = F.softmax(score, dim=1)  # softmax를 통해 확률분포값으로 변환\n",
    "        return  normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:37:25.485826Z",
     "start_time": "2020-10-22T05:37:25.461285Z"
    }
   },
   "outputs": [],
   "source": [
    "### Decoder 모듈 정의 ####\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, dec_ntoken: int, dropout: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim # Decoder RNN의 previous hidden\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(enc_hid_dim=hidden_dim, \n",
    "                                   dec_hid_dim=hidden_dim, \n",
    "                                   attn_dim=hidden_dim) # attention layer\n",
    "        \n",
    "        self.dec_ntoken = dec_ntoken # tgt vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(dec_ntoken, hidden_dim, \n",
    "                                      padding_idx=tgt.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # bidirectinal=False 임.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(self.hidden_dim*3, dec_ntoken) # Vocab 크기로 linear projection\n",
    "        self.sm = nn.LogSoftmax(dim=-1) # 확률 분포 값.\n",
    "\n",
    "    def _context_rep(self, dec_out, enc_outs):\n",
    "        scores = self.attention(dec_out, enc_outs) # score = [B, src_len]\n",
    "        scores = scores.unsqueeze(1) # [B, 1, src_len] -> weight value (softmax)\n",
    "\n",
    "        # scores: (batch, 1, src_len),  ecn_outs: (Batch, src_len, dim)\n",
    "        context_vector = torch.bmm(scores, enc_outs) # weighted average -> (batch, 1, dec_dim): encoder의 각 hidden의 weighted sum\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        dec_outs = []\n",
    "        embedded = self.dropout(self.embedding(input)) # (Batch, length, Dim)\n",
    "        \n",
    "        # (Batch, 1, dim)  (batch, 1, dim) , ....,\n",
    "        for emb_t in embedded.split(1, dim=1): # Batch 별 각 단어 (=각 time step) 에 대한 embedding 출력 \n",
    "            rnn_out, decoder_hidden = self.rnn(emb_t, decoder_hidden) # feed input with previous decoder hidden at each step\n",
    "\n",
    "            context = self._context_rep(rnn_out, encoder_outputs) # C_t vector\n",
    "            rnn_context = self.dropout(torch.cat([rnn_out, context], dim=2)) \n",
    "            dec_out = self.linear(rnn_context) # W(H + C) \n",
    "            dec_outs += [self.sm(dec_out)]\n",
    "        \n",
    "        if len(dec_outs) > 1:\n",
    "            dec_outs = dec_outs[:-1] # trg = trg[:-1] # <E> 는 Decoder 입력으로 고려하지 않음.\n",
    "            dec_outs = torch.cat(dec_outs, dim=1) # convert list into tensor : [B, L, vocab]\n",
    "\n",
    "        else: # step-wise 로 decoding 하는 경우,\n",
    "            dec_outs = dec_outs[0] # [B=1, L=1, vocab]\n",
    "\n",
    "        return dec_outs, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:38:06.657449Z",
     "start_time": "2020-10-22T05:38:06.646588Z"
    }
   },
   "outputs": [],
   "source": [
    "### Seq-to-Seq 모델 정의 ###\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src) # encoder_outputs = (Batch, length, Dim * 2) , hidden = (Batch, Dim)\n",
    "        dec_out, _ = self.decoder(trg, hidden, encoder_outputs)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:46:01.524075Z",
     "start_time": "2020-10-22T05:46:01.507999Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src.vocab)  # src 사전 크기\n",
    "OUTPUT_DIM = len(tgt.vocab) # tgt 사전 크기\n",
    "HID_DIM = 128 # rnn, embedding, 등. 모든 hidden 크기를 해당 값으로 통일함. (실습의 용이성을 위함.)\n",
    "D_OUT = 0.1 # Dropout  확률\n",
    "BATCH_SIZE = 26\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                       sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:46:11.789543Z",
     "start_time": "2020-10-22T05:46:11.596823Z"
    }
   },
   "outputs": [],
   "source": [
    "# 인코더 및 디코더 생성\n",
    "# Seq2Seq 모델 생성\n",
    "encoder = Encoder(HID_DIM, INPUT_DIM, D_OUT)\n",
    "decoder = Decoder(HID_DIM, OUTPUT_DIM, D_OUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:47:32.598320Z",
     "start_time": "2020-10-22T05:47:32.568114Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights) # 모델 파라미터 초기화\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Optimizer 설정\n",
    "criterion = nn.NLLLoss(ignore_index=tgt.vocab.stoi['<pad>'], reduction='mean') # LOSS 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:47:57.158820Z",
     "start_time": "2020-10-22T05:47:57.149092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(13296, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (merge): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(15004, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear): Linear(in_features=384, out_features=15004, bias=True)\n",
      "    (sm): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The model has 9,778,461 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 모델 정보 및 파라미터 수 출력\n",
    "def count_parameters(model: nn.Module):\n",
    "    print(model)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T05:49:37.920243Z",
     "start_time": "2020-10-22T05:49:37.906705Z"
    }
   },
   "outputs": [],
   "source": [
    "## 모델 학습 함수 ###\n",
    "def train(model, iterator, optimize, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt) # [batch, length, vocab_size]\n",
    "        output = output.view(-1, output.size(-1)) # flatten --> (batch * length, vocab_size)\n",
    "\n",
    "        tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # 정답에는 <S>가 포함되지 않음으로, 이를 삭제\n",
    "        tgt = tgt.view(-1) # flatten = (batch * length)\n",
    "\n",
    "        loss = criterion(output, tgt) # tgt 이 내부적으로 one_hot으로 변환됨 --> (batch * length, vocab_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if(((i+1) % int(len(iterator)*0.2)) == 0):\n",
    "            num_complete = batch.batch_size * (i+1)\n",
    "            total_size = batch.batch_size * int(len(iterator))\n",
    "            ratio = num_complete/total_size * 100\n",
    "            print('| Current Epoch:  {:>4d} / {:<5d} ({:2d}%) | Train Loss: {:3.3f}'.\n",
    "                  format(num_complete, batch.batch_size * int(len(iterator)), round(ratio), loss.item())\n",
    "                  )\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T06:29:46.943406Z",
     "start_time": "2020-10-22T06:29:46.928222Z"
    }
   },
   "outputs": [],
   "source": [
    "### 모델 평가 함수 ###\n",
    "def evaluate(model: nn.Module, iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.tgt\n",
    "\n",
    "            output = model(src, tgt)\n",
    "            output = output.view(-1, output.size(-1)) # flatten (batch * length, vocab_size)\n",
    "\n",
    "            tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # remove <S> placed at first from targets\n",
    "            tgt = tgt.view(-1) # flatten target with shape = (batch * length)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T06:29:47.205914Z",
     "start_time": "2020-10-22T06:29:47.198673Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습 시간 카운트를 위한 함수 #\n",
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T06:43:58.172655Z",
     "start_time": "2020-10-22T06:29:48.024355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 4.686\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 3.799\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 3.379\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 3.193\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 2.598\n",
      "=================================================================\n",
      "Epoch: 01 | Time: 4m 57s\n",
      "\tTrain Loss: 3.791 | Train PPL:  44.300\n",
      "\t Val. Loss: 2.773 |  Val. PPL:  16.001\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 2.662\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 2.399\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 2.310\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.899\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 2.073\n",
      "=================================================================\n",
      "Epoch: 02 | Time: 4m 49s\n",
      "\tTrain Loss: 2.288 | Train PPL:   9.853\n",
      "\t Val. Loss: 1.990 |  Val. PPL:   7.315\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.823\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 2.126\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 2.112\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-70fc7e2444fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3f6f4f12b164>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimize, criterion, clip)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tgt 이 내부적으로 one_hot으로 변환됨 --> (batch * length, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15 # 최대 epoch 크기\n",
    "CLIP = 0.2 # weight cliping \n",
    "isTrain = True # True 인 경우 아래 학습 코드 실행, False인 경우 저장된 model 로드만 수행.\n",
    "\n",
    "if isTrain:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print('='*65)\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print('='*65)\n",
    "\n",
    "    with open('NMT.pt', 'wb') as f:\n",
    "        print(\"model saving..\")\n",
    "        torch.save(model, f)\n",
    "\n",
    "else:\n",
    "    with open('NMT.pt', 'rb') as f:\n",
    "        model = torch.load(f).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T06:43:58.175284Z",
     "start_time": "2020-10-22T06:43:45.642Z"
    }
   },
   "outputs": [],
   "source": [
    "## Greedy decoding \n",
    "def greedy_decoding(model, input, fields, maxLen=20):\n",
    "    # input은 한 문장\n",
    "    src_field = [('src', fields[0])]\n",
    "    tgt_field = fields[1]\n",
    "\n",
    "    ex = Example.fromlist([input], src_field) # field에 정의된 내용으로 전처리 (tokenizing) 수행\n",
    "    src_tensor = src.numericalize([ex.src], device) # torch.Tensor로 치환, indexing, bos, eos 등의 처리과정도 함께 적용됨.\n",
    "    tgt_tensor = torch.tensor([[tgt_field.vocab.stoi['<s>']]], device=device) # Decoder 초기 입력 \n",
    "    model.eval()\n",
    "\n",
    "    dec_result = []\n",
    "    with torch.no_grad():\n",
    "        enc_out, hidden = model.encoder(src_tensor)\n",
    "        for i in range(maxLen):\n",
    "            # Step1: tgt_tensor (입력) 과 인코더의 출력을 이용하여 디코더 결과 출력\n",
    "            # Do someting about Step1 here..\n",
    "            # --> dec_step, hidden = model.decoder(....)\n",
    "            dec_step, hidden = model.decoder(tgt_tensor, hidden, enc_out)\n",
    "            \n",
    "            \n",
    "            # Step2: 디코더의 출력결과 (확룰분포) 에서 Top1 에 해당하는 word Index 추출\n",
    "            # Do someting about Step2 here..\n",
    "            # use torch.topk(..) \n",
    "            _, top_idx = torch.topk(dec_step, 1) # topk: return value, index\n",
    "            \n",
    "            # Step3: \n",
    "            # if: 출력된 word Index == EOS 인 경우 디코딩 중지 (break).\n",
    "            # else: 출력된 word Index를 저장하고, 다음 step의 디코더 입력 (tgt_tensor)으로 전달\n",
    "            if tgt_field.vocab.itos[top_idx] == '</s>':\n",
    "                break\n",
    "            else:\n",
    "                dec_result.append(top_idx.item()) # .item() : tensor 형태를 단순 integer로 \n",
    "                tgt_tensor = top_idx.view(1, 1)\n",
    "           \n",
    "    dec_result = [tgt_field.vocab.itos[w] for w in dec_result] # Word index를 단어로 치환\n",
    "    return dec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T06:43:58.177193Z",
     "start_time": "2020-10-22T06:43:51.644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Greedy decoding 수행\n",
    "\n",
    "input_sent = input('Enter a english sentence:  ')\n",
    "output = greedy_decoding(model, input_sent, fields=(src, tgt))\n",
    "output = MosesDetokenizer('fr')(output)\n",
    "print('> ', input_sent)\n",
    "print('< ', output)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
