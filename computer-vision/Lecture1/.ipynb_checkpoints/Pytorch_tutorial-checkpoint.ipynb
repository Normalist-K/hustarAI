{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power of **Pytorch** 1 -  Similar to Numpy\n",
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_h = torch.randn(20, 20, requires_grad=True)\n",
    "W_x = torch.randn(20, 10, requires_grad=True)\n",
    "x = torch.randn(1,10)\n",
    "prev_h = torch.randn(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3259)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2h = torch.matmul(W_h, prev_h.t())\n",
    "i2h = torch.matmul(W_x, x.t())\n",
    "\n",
    "next_h = h2h + i2h\n",
    "next_h = torch.tanh(next_h)\n",
    "\n",
    "loss = next_h.sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32594818"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_h_numpy = W_h.detach().numpy()\n",
    "prev_h_numpy = prev_h.numpy()\n",
    "\n",
    "## TO DO\n",
    "## First, copy the above code and change what...?\n",
    "h2h = np.matmul(W_h_numpy, prev_h_numpy.T)\n",
    "i2h = np.matmul(W_x.detach().numpy(), x.numpy().T)\n",
    "\n",
    "next_h = h2h + i2h\n",
    "next_h = np.tanh(next_h)\n",
    "\n",
    "loss = next_h.sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power of **Pytorch** 2 - Comparison of frameworks\n",
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ],\n",
       "       [ 1.86755799, -0.97727788,  0.95008842, -0.15135721],\n",
       "       [-0.10321885,  0.4105985 ,  0.14404357,  1.45427351]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "grad_c = 1.0\n",
    "## TO DO : Calculate manually gradients\n",
    "grad_b = grad_c * np.ones((N, D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x\n",
    "grad_y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namyup/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/namyup/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/namyup/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/namyup/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = tf.placeholder(tf.float64)\n",
    "y = tf.placeholder(tf.float64)\n",
    "z = tf.placeholder(tf.float64)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = tf.reduce_sum(b)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "## TO DO : use gradient function in Tensorflow \n",
    "grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    np.random.seed(0)\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        z: np.random.randn(N, D),\n",
    "  \n",
    "  }\n",
    "    # TO DO : run session\n",
    "    out = sess.run([c, grad_x, grad_y, grad_z], feed_dict=values)\n",
    "    c_v, grad_x_v, grad_y_v, grad_z_v = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = torch.randn(N, D, requires_grad=True)\n",
    "y = torch.randn(N, D, requires_grad=True)\n",
    "z = torch.randn(N, D, requires_grad=True)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Tutorial\n",
    "- Indexing 과 broadcasting 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Array indexing\n",
    "* Numpy array의 indexing은 일반적인 list와 유사하다. \n",
    "* 단, indexing해서 분리한 array도 원래 array의 memory를 참조하기 때문에 변경할 때 유의하여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]] \n",
      "\n",
      "[[2 3]\n",
      " [6 7]]\n",
      "2\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(a, '\\n')\n",
    "\n",
    "# [[2 3]\n",
    "#  [6 7]]\n",
    "# call-by-reference가 된 경우\n",
    "b = a[:2, 1:3]\n",
    "print(b)\n",
    "\n",
    "print(a[0, 1])\n",
    "b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]\n",
    "print(a[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  Slicing을 할 때는 dimension이 낮아질 수 있다.\n",
    "- Slicing을 하는 방법에는 여러가지가 있는데, integer를 활용해 indexing을 할 때는 dimension이 낮아지고, slicing을 이용해 indexing 할 때는 dimension이 유지된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]] (3, 4)\n",
      "Slicing Row\n",
      "[5 6 7 8] (4,)\n",
      "[[5 6 7 8]] (1, 4)\n",
      "[[5 6 7 8]] (1, 4)\n",
      "Slicing Column\n",
      "[ 2  6 10] (3,) \n",
      "\n",
      "[[ 2]\n",
      " [ 6]\n",
      " [10]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create the following rank 2 array with shape (3, 4)\n",
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(a, a.shape)\n",
    "\n",
    "row_r1 = a[1, :]    # Rank 1 view of the second row of a  \n",
    "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
    "row_r3 = a[[1], :]  # Rank 2 view of the second row of a\n",
    "# row_r1 = np.expand_dims(row_r1, axis=0)\n",
    "print(\"Slicing Row\")\n",
    "print(row_r1, row_r1.shape)\n",
    "print(row_r2, row_r2.shape)\n",
    "print(row_r3, row_r3.shape)\n",
    "\n",
    "\n",
    "col_r1 = a[:, 1]\n",
    "col_r2 = a[:, 1:2]\n",
    "print(\"Slicing Column\")\n",
    "print(col_r1, col_r1.shape, '\\n')\n",
    "print(col_r2, col_r2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer array를 이용해 indexing을 할 수 있다. \n",
    "- Slicing을 할 때는 네모난 subarray만 추출할 수 있지만, integer array를 이용할 경우 임의의 수치들을 꺼내올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 5]\n",
      "[1 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2], [3, 4], [5, 6]])\n",
    "\n",
    "print(np.array([a[0, 0], a[1, 1], a[2, 0]]))\n",
    "print(a[[0, 1, 2], [0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[ 1  6  7 11]\n",
      "[[11  2  3]\n",
      " [ 4  5 16]\n",
      " [17  8  9]\n",
      " [10 21 12]]\n"
     ]
    }
   ],
   "source": [
    "# Create a new array from which we will select elements\n",
    "a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "print(a)\n",
    "\n",
    "## TO DO \n",
    "# Select one element from each row of a using the indices\n",
    "b = np.array([0, 2, 0, 1])\n",
    "print(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\"\n",
    "\n",
    "a[np.arange(4), b] += 10\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean array로도 indexing을 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "[11  3  4  5 16 17  8  9 10 21 12]\n"
     ]
    }
   ],
   "source": [
    "print(a > 2)\n",
    "print(a[a > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "- Broadcasting is strong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = np.array([1, 0, 1])\n",
    "y = np.empty_like(x)   \n",
    "\n",
    "for i in range(4):\n",
    "    y[i, :] = x[i, :] + v\n",
    "print(y)\n",
    "\n",
    "vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\n",
    "y = x + vv  \n",
    "print(y)\n",
    "\n",
    "y = x + v  # Add v to each row of x using broadcasting\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong\n",
      "correct\n",
      "correct\n",
      "wrong\n"
     ]
    }
   ],
   "source": [
    "##Quiz\n",
    "def checkbroadcasting(x, y):\n",
    "    try:\n",
    "        x+y\n",
    "        print(\"correct\")\n",
    "    except:\n",
    "        print(\"wrong\")\n",
    "\n",
    "x=np.empty((0))\n",
    "y=np.empty((2,2))\n",
    "checkbroadcasting(x,y)\n",
    "        \n",
    "x=np.empty((5,3,4,1))\n",
    "y=np.empty((3,4,1))\n",
    "checkbroadcasting(x,y)\n",
    "\n",
    "x=np.empty((5,3,4,1))\n",
    "y=np.empty((3,1,1))\n",
    "checkbroadcasting(x,y)\n",
    "\n",
    "x=np.empty((5,2,4,1))\n",
    "y=np.empty((3,1,1))\n",
    "checkbroadcasting(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지금까지 배운 indexing 과 Broadcasting 방법이 ***모두*** Pytorch에도 적용 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "* Tensorflow의 Tensor와 다르지 않다.\n",
    "  * Numpy의 ndarrays를 기본적으로 활용하고 있다.\n",
    "  * Numpy의 ndarrays의 대부분의 operation을 사용할 수 있도록 구성되어 있다.\n",
    "* Numpy의 operation은 CPU만을 이용해 느리지만 Tensor는 CUDA를 활용해 GPU를 이용하기 때문에 빠르게 연산을 진행할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7902e+14,  4.5902e-41, -9.1801e+22],\n",
      "        [ 3.0726e-41,  4.4842e-44,  1.2752e-43],\n",
      "        [ 6.8664e-44,  6.7262e-44,  6.1657e-44],\n",
      "        [ 4.4842e-44,  6.8664e-44,  6.8664e-44],\n",
      "        [ 6.1657e-44,  4.4842e-44,  2.0319e-43]]) \n",
      "\n",
      "tensor([[ 0.4945,  0.6191,  0.4551],\n",
      "        [ 0.9875,  0.2229,  0.9673],\n",
      "        [ 0.4188,  0.4648,  0.8254],\n",
      "        [ 0.4883,  0.4903,  0.0448],\n",
      "        [ 0.1804,  0.3096,  0.0294]]) \n",
      "\n",
      "tensor([[ 3,  4,  5],\n",
      "        [ 1,  2,  3]]) \n",
      "\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Construct a 5 x 3 matrix, uninitialized\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a randomly initialized matrix \n",
    "x = torch.rand(5, 3)\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a matrix with the list\n",
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]])\n",
    "print(x, '\\n')\n",
    "\n",
    "# Get its size\n",
    "print(x.size())\n",
    "print(x.shape) #???? is it Numpy? kkkkk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtype and device \n",
    " * dtype - Tensor의 데이터 타입\n",
    " * device - Tensor의 작업 위치 (cpu or cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]], dtype=torch.float64) \n",
      "\n",
      "tensor([[3, 4, 5],\n",
      "        [1, 2, 3]]) \n",
      "\n",
      "tensor([[ 6.,  8., 10.],\n",
      "        [ 2.,  4.,  6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]], dtype=torch.float64)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = torch.tensor([[3, 4, 5], [1, 2, 3]])\n",
    "print(y, '\\n')\n",
    "\n",
    "#error\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  4.,  5.],\n",
      "        [ 1.,  2.,  3.]], dtype=torch.float64) \n",
      "\n",
      "tensor([[  6.,   8.,  10.],\n",
      "        [  2.,   4.,   6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = y.double() \n",
    "print(y, '\\n')\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  4.,  5.],\n",
      "        [ 1.,  2.,  3.]], dtype=torch.float64, device='cuda:1') \n",
      "\n",
      "cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "device = torch.device('cuda:1')\n",
    "x = x.to(device)\n",
    "\n",
    "print(x, '\\n')\n",
    "print(x.device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before \"to\" method\n",
      "torch.float64 cpu\n",
      "torch.float32 cpu\n",
      "torch.int32 cuda:1 \n",
      "\n",
      "After \"to\" method\n",
      "torch.int32 cuda:0\n",
      "torch.int32 cuda:1\n",
      "torch.int32 cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device_0 = torch.device('cuda:0')\n",
    "device_1 = torch.device('cuda:1')\n",
    "\n",
    "x = torch.randn(4, 3, dtype=torch.float64)\n",
    "y = torch.randn(4, 3, dtype=torch.float32)\n",
    "z = torch.randint(0, 10, (4, 3), dtype=torch.int32)\n",
    "\n",
    "z = z.to(device_1)\n",
    "\n",
    "print('Before \"to\" method')\n",
    "\n",
    "print(x.dtype, x.device)\n",
    "print(y.dtype, y.device)\n",
    "print(z.dtype, z.device, '\\n')\n",
    "\n",
    "print('After \"to\" method')\n",
    "# to method with specific dtype and device \n",
    "x = x.to(dtype=torch.int32, device=device_0)\n",
    "\n",
    "# to method with some tensor \n",
    "y = y.to(z)\n",
    "z = z.to(device=\"cpu\")\n",
    "\n",
    "print(x.dtype, x.device)\n",
    "print(y.dtype, y.device)\n",
    "print(z.dtype, z.device, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing like Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7902e+14,  4.5902e-41,  3.7902e+14,  4.5902e-41,         nan],\n",
      "        [ 0.0000e+00,  7.6194e+31,  1.5564e+28,  1.8484e+31,  1.8370e+25],\n",
      "        [ 1.4603e-19,  2.7517e+12,  7.5338e+28,  3.0313e+32,  6.3828e+28]]) \n",
      "\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.]]) \n",
      "\n",
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]]) \n",
      "\n",
      "tensor([[ 3.1415,  3.1415,  3.1415,  3.1415,  3.1415],\n",
      "        [ 3.1415,  3.1415,  3.1415,  3.1415,  3.1415],\n",
      "        [ 3.1415,  3.1415,  3.1415,  3.1415,  3.1415]]) \n",
      "\n",
      "tensor([ 0.,  2.,  4.]) \n",
      "\n",
      "tensor([ 0.0000,  0.6250,  1.2500,  1.8750,  2.5000,  3.1250,  3.7500,\n",
      "         4.3750,  5.0000]) \n",
      "\n",
      "tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) \n",
      "\n",
      "tensor([[ 1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.]]) \n",
      "\n",
      "tensor([[ 9.,  6.,  5.,  6.,  3.],\n",
      "        [ 5.,  8.,  6.,  7.,  4.],\n",
      "        [ 9.,  6.,  6.,  6.,  3.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.zeros(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.ones(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.full((3, 5), 3.1415)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.arange(0, 5, 2)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = torch.linspace(0, 5, 9)\n",
    "print(y, '\\n')\n",
    "\n",
    "z = torch.logspace(-10, 10, 5)\n",
    "print(z, '\\n')\n",
    "\n",
    "z = torch.eye(5)\n",
    "print(z, '\\n')\n",
    "\n",
    "# Construct a 3 x 5 matrix with random value from uniform distribution, i.e. Uniform[0, 1)\n",
    "x = torch.rand(3, 5)\n",
    "\n",
    "# Construct a 3 x 5 matrix with random value from normal distribution, i.e. Normal(0, 1)\n",
    "x = torch.randn(3, 5)\n",
    "\n",
    "x = torch.randint(3, 10, (3, 5))\n",
    "print(x, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*\\_like function and new\\_\\* function\n",
    " * \\*\\_like: Tensor를 input으로 받아, Tensor 모양의 matrix를 return.\n",
    " * new\\_\\*: Shape를 input으로 받아, Tensor와 같은 type과 device를 가지는 matrix를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.]]) \n",
      "\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]]) \n",
      "\n",
      "torch.float32 cpu\n",
      "torch.float32 cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros_like(x)\n",
    "print(y, '\\n')\n",
    "\n",
    "# Make zero matrix with attribute of x\n",
    "z = x.new_zeros(2, 3)\n",
    "print(z, '\\n')\n",
    "print(x.dtype, x.device)\n",
    "print(z.dtype, z.device,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [1. 1. 1. 1. 1.] \n",
      " tensor([ 1.,  1.,  1.,  1.,  1.], dtype=torch.float64) \n",
      " [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "c = b.numpy()\n",
    "print(\"\\n\",a,\"\\n\",b,\"\\n\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "* Operations에도 여러가지 syntax가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4239,  1.1097,  1.1231],\n",
      "        [ 1.8313,  1.3736,  1.4430],\n",
      "        [ 0.1173,  1.6869,  1.2679],\n",
      "        [ 0.3231,  1.1983,  1.8849],\n",
      "        [ 1.0036,  0.9917,  1.2875]]) \n",
      "\n",
      "tensor([[ 0.4239,  1.1097,  1.1231],\n",
      "        [ 1.8313,  1.3736,  1.4430],\n",
      "        [ 0.1173,  1.6869,  1.2679],\n",
      "        [ 0.3231,  1.1983,  1.8849],\n",
      "        [ 1.0036,  0.9917,  1.2875]]) \n",
      "\n",
      "tensor([[ 0.4239,  1.1097,  1.1231],\n",
      "        [ 1.8313,  1.3736,  1.4430],\n",
      "        [ 0.1173,  1.6869,  1.2679],\n",
      "        [ 0.3231,  1.1983,  1.8849],\n",
      "        [ 1.0036,  0.9917,  1.2875]]) \n",
      "\n",
      "tensor([[ 0.4239,  1.1097,  1.1231],\n",
      "        [ 1.8313,  1.3736,  1.4430],\n",
      "        [ 0.1173,  1.6869,  1.2679],\n",
      "        [ 0.3231,  1.1983,  1.8849],\n",
      "        [ 1.0036,  0.9917,  1.2875]]) \n",
      "\n",
      "tensor([ 0.8792,  0.9695,  0.9166,  0.6151,  0.1074]) \n",
      "\n",
      "tensor([ 0.8792,  0.9021,  0.9695,  0.7801,  0.9166,  0.6151,  0.9025,\n",
      "         0.7320])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y, '\\n')\n",
    "\n",
    "print(torch.add(x, y), '\\n')\n",
    "\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result, '\\n')\n",
    "\n",
    "y.add_(x)\n",
    "print(y, '\\n')\n",
    "\n",
    "# indexing 또한 비슷하게\n",
    "print(x[:, 1], '\\n')\n",
    "print(x[x > 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape -> view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]) \n",
      "\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.]]) \n",
      "\n",
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.],\n",
      "        [  6.,   7.,   8.,   9.,  10.,  11.],\n",
      "        [ 12.,  13.,  14.,  15.,  16.,  17.],\n",
      "        [ 18.,  19.,  20.,  21.,  22.,  23.],\n",
      "        [ 24.,  25.,  26.,  27.,  28.,  29.]]) \n",
      "\n",
      "torch.Size([5, 6]) \n",
      "\n",
      "tensor([[[  0.,   1.,   2.,   3.,   4.],\n",
      "         [  5.,   6.,   7.,   8.,   9.]],\n",
      "\n",
      "        [[ 10.,  11.,  12.,  13.,  14.],\n",
      "         [ 15.,  16.,  17.,  18.,  19.]],\n",
      "\n",
      "        [[ 20.,  21.,  22.,  23.,  24.],\n",
      "         [ 25.,  26.,  27.,  28.,  29.]]]) \n",
      "\n",
      "torch.Size([3, 2, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the shape of tensor \n",
    "x = torch.arange(0, 10)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = x.view(2, 5)\n",
    "print(y, '\\n')\n",
    "\n",
    "x = torch.arange(0, 30).view(5, 6)\n",
    "print(x, '\\n')\n",
    "print(x.size(), '\\n')\n",
    "\n",
    "y = x.view(-1, 2, 5)\n",
    "print(y, '\\n')\n",
    "print(y.size(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expand_dim -> unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.]]) \n",
      "\n",
      "tensor([[ 0.,  5.],\n",
      "        [ 1.,  6.],\n",
      "        [ 2.,  7.],\n",
      "        [ 3.,  8.],\n",
      "        [ 4.,  9.]]) torch.Size([5, 2]) \n",
      "\n",
      "torch.Size([1, 2, 5]) \n",
      "\n",
      "torch.Size([2, 1, 5]) \n",
      "\n",
      "torch.Size([2, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the dimension of tensor\n",
    "x = torch.arange(0, 10).view(2, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "# view를 사용하기 까다로울때 사용\n",
    "y = x.permute(1, 0)\n",
    "print(y, y.shape, '\\n')\n",
    "\n",
    "# Add the dimension of tensor \n",
    "z = x.unsqueeze(0)\n",
    "print(z.size(), '\\n')\n",
    "\n",
    "z = x.unsqueeze(1)\n",
    "print(z.size(), '\\n')\n",
    "\n",
    "# size=1 인 dimension 제거\n",
    "z = z.squeeze()\n",
    "print(z.size(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiplication and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([5, 3])\n",
      "tensor([[ 6.,  6.,  6.],\n",
      "        [ 6.,  6.,  6.],\n",
      "        [ 6.,  6.,  6.],\n",
      "        [ 6.,  6.,  6.],\n",
      "        [ 6.,  6.,  6.]]) torch.Size([5, 3])\n",
      "tensor([[ 18.,  18.,  18.,  18.,  18.],\n",
      "        [ 18.,  18.,  18.,  18.,  18.],\n",
      "        [ 18.,  18.,  18.,  18.,  18.],\n",
      "        [ 18.,  18.,  18.,  18.,  18.],\n",
      "        [ 18.,  18.,  18.,  18.,  18.]]) torch.Size([5, 5])\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones( 5, 3)+1\n",
    "y = torch.ones( 5, 3)+2\n",
    "z = x * y\n",
    "print(x.shape, y.shape)\n",
    "print(z, z.shape)\n",
    "\n",
    "z= torch.matmul(x, y.t())\n",
    "print(z, z.shape)\n",
    "\n",
    "x = x.unsqueeze(0)\n",
    "y = y.unsqueeze(0)\n",
    "z = torch.cat([x, y], dim=0)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tile -> expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 700, 28])\n",
      "torch.Size([100, 28, 700])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100, 700)\n",
    "x = x.unsqueeze(2).expand(100, 700, 28)\n",
    "print(x.shape)\n",
    "\n",
    "x = torch.randn(100, 700)\n",
    "x = x.unsqueeze(1).expand(100, 28, 700)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\***********Numpy Practice Time\\***********\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = torch.ones(4,5,4,4)\n",
    "ch_insert= torch.ones(4,2,4,4)*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]]],\n",
       "\n",
       "\n",
       "        [[[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]]],\n",
       "\n",
       "\n",
       "        [[[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]]],\n",
       "\n",
       "\n",
       "        [[[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.],\n",
       "          [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_uns2 = src.unsqueeze(2)\n",
    "ch_insert_uns1 = ch_insert.unsqueeze(1)\n",
    "expand_dim = list(src_uns2.shape)\n",
    "expand_dim[2] = ch_insert_uns1.shape[2]\n",
    "ch_insert_uns1 = ch_insert_uns1.expand(tuple(expand_dim))\n",
    "cated_tensor = torch.cat([ch_insert_uns1, src_uns2], dim=2)\n",
    "result = cated_tensor.view(expand_dim[0],expand_dim[1]*(expand_dim[2]+1),expand_dim[3],expand_dim[4])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: automatic differentiation\n",
    "* Autograd package는 Tensors가 사용할 수 있는 모든 Operation의 Gradient를 자동으로 계산해준다.\n",
    "* Tensor의 required_grad attribute를 이용해 gradient의 계산여부를 결정할 수 있다.\n",
    "  * 계산이 완료된 이후에 .backward()를 호출하면 자동으로 gradient를 계산한다.\n",
    "  * .grad attribute를 통해 마찬가지로 gradient에 접근할 수 있다. \n",
    "  * .grad_fn attribute를 통해 해당 Variable이 어떻게 생성되었는지 확인할 수 있다.\n",
    "  \n",
    "  \n",
    "![Alt text](./resource/Variable.png \"Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create a variable\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n",
      "None\n",
      "<AddBackward0 object at 0x7ff4e0bf9630>\n",
      "tensor([[ 27.,  27.],\n",
      "        [ 27.,  27.]]) <MulBackward0 object at 0x7ff4e0bf9b38> \n",
      "\n",
      "tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "# y는 operation으로 생성된 결과이기 때문에 grad_fn이 있지만 , x는 없다.\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "\n",
    "# Do more operations on y \n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, z.grad_fn, '\\n')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradients \n",
    "* out.backward()을 하면 out의 gradient를 1로 시작해 Back-propagation을 시작한다.\n",
    "* .backward()를 호출한 이후부터는 .grad를 통해 각 변수의 gradient를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5000,  4.5000],\n",
      "        [ 4.5000,  4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# out.backward() == out.backward(tr.Tensor([1.0]))\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실제로 Gradient 를 계산하면 다음과 같다.\n",
    "$$o = \\frac{1}{4}\\sum_{i} z_{i}$$ \n",
    "\n",
    "$$z_{i}=3(x_{i}+2)^{2}$$\n",
    "\n",
    "$$z_{i}|_{x_{i}=1} = 27 $$\n",
    "\n",
    "$$ \\frac{\\partial o}{\\partial x_{i}} = \\frac{3}{2}(x_{i} + 2) $$\n",
    "\n",
    "$$ \\frac{\\partial o}{\\partial x_{i}}|_{x_{i}=1} = 4.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1126.1876,   563.5027,  -415.0867])\n",
      "tensor([  102.4000,  1024.0000,     0.1024])\n"
     ]
    }
   ],
   "source": [
    "# We can do many crazy thing with autograd\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## 1. Define the network\n",
    "* nn.Module 을 inherit하는 class를 define한다.\n",
    "  * __init__(self): 생성자, network에서 사용할 구조를 정의한다.\n",
    "  * forward(self, x): x를 input으로 받는 network가 어떻게 작동해 어떤 output을 내놓을지 정의한다.\n",
    "  * backward(self, grad_output): grad_output을 받아 직전 layer로 gradient를 backpropagation해주는 함수. 기본적으로는 autograd를 통해 자동으로 정의된다.\n",
    "  \n",
    "  ![convnet](./resource/mnist.png \"Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 0.1188, -0.0810, -0.1891,  0.1806, -0.1064],\n",
      "          [ 0.1514, -0.0497, -0.1599,  0.0064, -0.1457],\n",
      "          [ 0.1492, -0.1378,  0.1424, -0.1847, -0.0252],\n",
      "          [ 0.1127,  0.1019,  0.1606, -0.1728,  0.1220],\n",
      "          [-0.0394, -0.0894, -0.1095,  0.1224,  0.1795]]],\n",
      "\n",
      "\n",
      "        [[[-0.0721, -0.1186,  0.1714, -0.0439,  0.0667],\n",
      "          [ 0.1964,  0.1168, -0.0411,  0.1811, -0.1519],\n",
      "          [ 0.0490,  0.1189,  0.0764, -0.1708, -0.1223],\n",
      "          [ 0.0162,  0.1985, -0.0964, -0.0022,  0.0408],\n",
      "          [-0.1962,  0.1946, -0.1186,  0.0171,  0.0280]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1752,  0.1841, -0.1559,  0.0077,  0.1884],\n",
      "          [ 0.1826, -0.1706,  0.1598,  0.1016,  0.0596],\n",
      "          [ 0.0939,  0.0499, -0.1368,  0.1171, -0.0868],\n",
      "          [ 0.1813,  0.1931, -0.1812, -0.1718,  0.0071],\n",
      "          [ 0.1554, -0.0506, -0.1698,  0.0270, -0.0489]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1803, -0.0773,  0.0824,  0.0639, -0.0777],\n",
      "          [ 0.0204, -0.0254,  0.0977,  0.1584,  0.0512],\n",
      "          [-0.0331,  0.1121,  0.1815, -0.1534,  0.0533],\n",
      "          [ 0.0399, -0.1453,  0.1209,  0.1168, -0.1979],\n",
      "          [-0.0493, -0.0129, -0.0872,  0.0058,  0.1103]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0527,  0.1840, -0.0581, -0.1774,  0.0456],\n",
      "          [-0.1712,  0.1170, -0.0196,  0.0721, -0.0269],\n",
      "          [-0.0532, -0.1514, -0.0016,  0.0802, -0.0565],\n",
      "          [ 0.1619,  0.1367,  0.0610, -0.1946,  0.1255],\n",
      "          [-0.1972, -0.1948,  0.0339,  0.1333, -0.0150]]],\n",
      "\n",
      "\n",
      "        [[[-0.0559, -0.0602,  0.1042,  0.0697,  0.1275],\n",
      "          [ 0.1179, -0.1580, -0.0944,  0.1197,  0.0066],\n",
      "          [-0.1477,  0.0550, -0.1199,  0.1442,  0.1780],\n",
      "          [-0.0647,  0.1394, -0.0169,  0.1954,  0.0826],\n",
      "          [ 0.0465,  0.1637, -0.1770,  0.1647, -0.0056]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0715,  0.0964, -0.0301,  0.0701,  0.0822, -0.1843],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 1.9073e-02, -1.3204e-02, -9.2375e-03,  6.7609e-02, -1.1635e-02],\n",
      "          [ 4.3101e-02, -3.2320e-02, -3.7770e-02,  7.5660e-02, -1.1790e-02],\n",
      "          [ 2.5066e-02,  1.1473e-02,  1.4562e-02, -2.3112e-02,  7.0768e-02],\n",
      "          [ 2.0288e-03,  8.8273e-03,  1.7332e-02, -4.9959e-02, -1.7124e-02],\n",
      "          [ 8.0255e-02, -6.1101e-02,  2.5676e-02, -2.8870e-02, -6.1600e-02]],\n",
      "\n",
      "         [[-2.5662e-02,  5.2159e-02,  2.3047e-02,  3.9069e-02,  5.0424e-02],\n",
      "          [-1.9965e-02, -6.8314e-02,  4.8525e-02, -7.5459e-02,  2.7205e-02],\n",
      "          [ 4.3215e-02,  1.1026e-02,  2.1137e-03, -3.2865e-02,  4.1711e-02],\n",
      "          [-7.1997e-02, -1.2792e-02,  4.7277e-02, -7.4262e-02, -6.5526e-02],\n",
      "          [-1.4344e-02, -2.4311e-02, -7.0236e-03,  1.1556e-02, -3.6903e-02]],\n",
      "\n",
      "         [[ 1.5839e-02, -5.1495e-03, -4.4809e-02,  7.3708e-02,  5.3055e-02],\n",
      "          [ 7.3577e-02, -3.6921e-02,  7.6785e-02, -4.8496e-03, -7.5920e-02],\n",
      "          [ 3.0199e-02, -7.1021e-02, -3.7746e-02, -3.8370e-02, -3.0607e-03],\n",
      "          [-6.7167e-02,  6.7459e-02, -3.9085e-02,  2.4974e-02,  6.8887e-02],\n",
      "          [-6.7064e-02, -3.3431e-02,  5.9983e-02, -1.0299e-02, -2.0357e-02]],\n",
      "\n",
      "         [[ 4.9866e-02,  5.2045e-03, -6.2794e-02, -2.1128e-02,  3.1205e-02],\n",
      "          [ 7.8375e-02, -4.3868e-02, -5.9182e-02, -2.4426e-02,  3.3561e-02],\n",
      "          [ 1.3963e-03,  5.7656e-02,  3.4432e-02, -2.3079e-02, -3.0053e-02],\n",
      "          [ 2.3687e-02, -6.5589e-02, -1.7328e-02,  2.7163e-02, -7.0818e-02],\n",
      "          [ 5.5655e-02, -6.4406e-02, -7.3418e-03,  5.9813e-02, -5.9347e-02]],\n",
      "\n",
      "         [[ 6.2240e-02,  3.4365e-02,  5.0361e-02,  6.3718e-02, -6.2222e-02],\n",
      "          [-6.5826e-02, -6.9016e-02, -9.3504e-03,  5.2939e-02, -4.8941e-02],\n",
      "          [ 3.6123e-02, -1.9723e-02,  1.6037e-02,  2.0831e-02,  6.8707e-02],\n",
      "          [-3.8755e-02, -3.4716e-02,  6.1016e-02,  6.4143e-02,  5.0553e-02],\n",
      "          [ 6.0594e-02,  6.3643e-02,  1.9494e-02,  6.5034e-02, -7.5885e-02]],\n",
      "\n",
      "         [[-2.2651e-02,  3.6584e-03,  7.9363e-02, -4.7498e-02,  3.9365e-03],\n",
      "          [ 6.3830e-02, -7.8824e-02, -3.4881e-02,  2.1460e-02,  5.9359e-03],\n",
      "          [-5.0013e-03,  3.4547e-02, -6.2529e-02,  1.5244e-02, -1.7769e-02],\n",
      "          [-6.5883e-02,  7.7576e-02,  4.5400e-02,  6.1040e-02, -4.5900e-02],\n",
      "          [-6.8677e-02, -2.6157e-02, -7.0930e-02,  9.4594e-03, -1.2066e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8097e-02, -6.0875e-02, -1.8587e-02, -7.0743e-03, -4.0485e-02],\n",
      "          [-2.6367e-02,  6.4149e-02,  6.9390e-03,  2.3135e-03, -5.4916e-02],\n",
      "          [-4.9546e-02, -7.2543e-02, -1.6167e-02, -4.0759e-02,  4.9526e-02],\n",
      "          [ 2.6632e-02, -6.6094e-02,  2.0693e-02, -2.8265e-03, -5.6709e-02],\n",
      "          [ 1.1863e-02, -8.0528e-03,  7.1909e-02, -1.3673e-02,  6.4692e-02]],\n",
      "\n",
      "         [[-1.3605e-02,  3.5390e-02, -9.4021e-03, -1.8171e-02,  2.2690e-02],\n",
      "          [ 3.9010e-02, -4.4973e-02, -2.0224e-02,  7.3227e-02,  2.4445e-03],\n",
      "          [-3.6710e-02, -6.8565e-03, -3.1503e-02, -5.1739e-02, -6.9756e-02],\n",
      "          [ 6.8372e-02, -7.9339e-02,  6.1353e-02, -6.0560e-03,  4.4082e-02],\n",
      "          [ 7.6518e-02,  5.6586e-02,  9.1901e-03, -3.5106e-02, -1.7651e-02]],\n",
      "\n",
      "         [[ 5.9714e-02,  1.9919e-02, -1.5310e-02,  1.9466e-03,  1.4278e-03],\n",
      "          [ 3.2888e-03,  5.7323e-03, -3.5482e-02, -1.3693e-03, -5.7455e-02],\n",
      "          [-1.5401e-02, -1.5106e-02,  3.4797e-02,  4.1919e-02,  7.9893e-02],\n",
      "          [-8.1254e-03,  4.8580e-02, -5.1200e-02,  5.3971e-02,  7.9472e-02],\n",
      "          [ 3.9754e-02,  2.8174e-02,  6.5365e-02,  2.2190e-02, -1.3269e-02]],\n",
      "\n",
      "         [[-8.0622e-02, -7.8711e-02, -2.5226e-02, -4.3005e-02, -2.2650e-02],\n",
      "          [-4.0159e-02, -8.8866e-03, -1.2696e-02,  2.9264e-03, -8.0190e-03],\n",
      "          [-6.5873e-02, -2.7070e-02,  2.2642e-02,  4.9397e-02, -4.1961e-02],\n",
      "          [-3.8630e-02, -1.7059e-02, -8.4162e-03,  1.4371e-02, -4.5187e-02],\n",
      "          [ 4.4593e-02, -3.3660e-02, -6.7280e-02,  6.5367e-02, -7.1612e-02]],\n",
      "\n",
      "         [[-1.8662e-02, -4.1238e-03, -2.4381e-02, -7.2219e-02,  2.4710e-03],\n",
      "          [-2.0662e-02,  2.6990e-02,  7.7341e-02, -1.0157e-02,  3.7387e-03],\n",
      "          [-2.2360e-02, -6.7111e-02, -1.4409e-02,  7.1656e-02, -8.1538e-02],\n",
      "          [-8.0171e-02, -4.1923e-02, -5.9568e-03, -7.5277e-02, -1.5383e-02],\n",
      "          [ 3.4168e-03, -1.7273e-02,  3.0905e-02, -5.7528e-02,  7.6617e-02]],\n",
      "\n",
      "         [[ 3.4823e-02, -3.3674e-02, -8.0576e-02,  3.9002e-03, -4.6577e-03],\n",
      "          [ 4.5776e-02,  3.7781e-02, -7.8082e-02,  2.8956e-02, -5.4573e-02],\n",
      "          [ 2.3917e-02, -8.0490e-02,  3.2692e-02,  8.0624e-02, -2.5988e-02],\n",
      "          [ 3.0889e-02,  2.8391e-02, -2.0166e-02,  5.6046e-02, -2.3862e-02],\n",
      "          [ 1.1614e-02, -3.3236e-02, -3.4237e-02, -2.4918e-02,  6.8284e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.5099e-02, -4.9164e-02, -2.0911e-02, -5.0201e-02, -1.4186e-02],\n",
      "          [-5.2256e-02, -8.0813e-02, -6.5870e-02, -1.4401e-02, -5.5762e-02],\n",
      "          [ 6.8246e-02, -2.0762e-02, -6.7086e-02, -4.6797e-02, -7.5313e-03],\n",
      "          [-4.3566e-02,  2.7505e-02,  9.9786e-03,  1.5891e-02,  4.6084e-02],\n",
      "          [-1.2582e-02, -3.5402e-02, -8.1302e-02, -7.6089e-02, -4.3048e-03]],\n",
      "\n",
      "         [[ 1.6165e-02, -3.2241e-02,  1.6483e-02,  5.6323e-02, -1.4228e-03],\n",
      "          [-6.8037e-02,  4.1492e-02, -4.6062e-02, -2.0337e-02, -3.7583e-02],\n",
      "          [ 4.8270e-02,  2.0395e-02, -6.5993e-03, -1.6144e-02,  4.5761e-02],\n",
      "          [ 6.5179e-02, -6.0609e-02, -9.2626e-05,  1.4270e-02,  7.7035e-02],\n",
      "          [-2.7446e-02,  7.2108e-02, -5.4145e-02, -9.4630e-03,  3.9431e-03]],\n",
      "\n",
      "         [[-4.1146e-02,  1.7110e-02, -2.8788e-02,  6.1563e-02, -7.0103e-02],\n",
      "          [-5.9623e-02, -3.4651e-02, -1.7881e-02, -5.1567e-02, -2.6730e-02],\n",
      "          [ 4.9265e-02, -5.4716e-02, -2.3864e-02, -8.0277e-02, -2.7589e-02],\n",
      "          [ 6.4324e-02, -3.8904e-03,  9.6202e-03, -2.4631e-02, -1.4079e-04],\n",
      "          [-7.4032e-03, -5.0333e-02, -6.5245e-02, -6.8517e-02,  6.8656e-02]],\n",
      "\n",
      "         [[-2.2528e-03,  3.3804e-02, -7.2856e-02,  4.2702e-02,  3.8264e-02],\n",
      "          [ 4.9909e-02,  7.1090e-02,  2.4794e-02, -4.7493e-02, -6.3827e-02],\n",
      "          [ 3.6265e-03, -6.2434e-02, -1.8841e-02, -1.1341e-02,  7.3903e-02],\n",
      "          [ 7.5416e-02,  3.6562e-02, -5.0656e-02, -3.3306e-02, -3.8431e-02],\n",
      "          [-4.5945e-02, -2.6211e-02,  3.3486e-02,  7.5778e-03, -7.0202e-02]],\n",
      "\n",
      "         [[ 6.4048e-02, -1.4850e-02, -1.5734e-02,  3.2595e-03, -7.2879e-02],\n",
      "          [-2.4256e-02, -5.2467e-02, -7.9027e-03, -6.2218e-02, -6.5387e-02],\n",
      "          [-2.8148e-02, -5.3469e-02, -6.8126e-02,  5.8237e-02, -4.3983e-02],\n",
      "          [-4.6320e-02,  5.6148e-02,  4.4931e-02,  3.9820e-04,  2.1013e-02],\n",
      "          [ 3.7362e-02,  1.0770e-03,  2.2215e-02,  6.6828e-02,  3.3859e-02]],\n",
      "\n",
      "         [[ 7.2706e-02,  1.8155e-03,  1.1154e-02, -2.7555e-02, -3.2610e-02],\n",
      "          [-7.5910e-02,  6.7371e-02,  2.6029e-02, -1.8511e-02, -5.3846e-03],\n",
      "          [-4.4199e-02, -5.0574e-02,  2.3166e-02, -5.3674e-02,  3.7117e-02],\n",
      "          [ 7.6150e-03, -2.2064e-02, -4.0731e-02,  3.3249e-02, -6.1217e-02],\n",
      "          [-1.5316e-02, -7.8406e-02,  3.3165e-02,  7.6219e-02, -6.6376e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.8672e-02, -6.3879e-02,  4.9132e-02, -2.6235e-02, -1.4329e-03],\n",
      "          [ 1.6508e-03, -3.6265e-02, -5.8615e-02, -5.7267e-02, -3.9128e-02],\n",
      "          [-1.5957e-02,  6.9231e-02, -5.0822e-02, -5.1090e-02,  2.0589e-02],\n",
      "          [ 7.0905e-02,  2.7276e-02,  4.9527e-02,  7.6421e-02,  3.8500e-03],\n",
      "          [ 2.7997e-02, -5.2096e-02, -6.9937e-02, -9.0023e-03, -4.1192e-03]],\n",
      "\n",
      "         [[ 7.0589e-02,  1.8951e-02, -6.4173e-02,  6.8602e-02,  5.2098e-02],\n",
      "          [ 7.6888e-02, -5.7639e-02,  5.4381e-02, -7.4173e-02, -5.5348e-02],\n",
      "          [ 6.8709e-02, -3.6457e-03,  7.9074e-03, -3.9924e-02, -3.8790e-03],\n",
      "          [-1.7393e-02, -3.3767e-02, -5.4132e-02, -6.4527e-02, -5.8901e-02],\n",
      "          [ 2.6983e-03,  6.9454e-02, -4.9190e-03,  6.5978e-02,  7.7367e-02]],\n",
      "\n",
      "         [[ 5.8352e-02,  2.9522e-02,  6.5706e-02, -7.4880e-02, -5.5143e-03],\n",
      "          [-3.2860e-02, -5.0127e-02, -4.8132e-02,  1.4302e-02,  2.8109e-02],\n",
      "          [ 1.1600e-02, -6.4138e-02, -1.8668e-02,  6.6602e-02, -1.7220e-02],\n",
      "          [ 4.7579e-02, -7.4044e-02, -3.5454e-02, -3.9673e-02,  4.2829e-02],\n",
      "          [ 3.2361e-02,  1.0091e-02,  1.0688e-02, -6.9118e-03, -3.6752e-02]],\n",
      "\n",
      "         [[-6.3395e-02, -2.6569e-02,  7.0191e-04,  1.9525e-02,  5.5688e-02],\n",
      "          [ 7.3592e-02, -5.3822e-02, -3.1976e-02,  4.6298e-02,  2.4320e-02],\n",
      "          [-1.4334e-02,  2.6557e-02, -4.5190e-02,  2.3497e-02, -6.1760e-02],\n",
      "          [ 2.9231e-02, -4.1472e-03, -5.0152e-02,  2.2752e-02, -4.3352e-02],\n",
      "          [-7.3212e-02,  4.4247e-03,  1.7856e-02, -1.5628e-02, -2.7594e-02]],\n",
      "\n",
      "         [[ 2.0312e-02, -9.2101e-04,  1.3990e-03, -2.9913e-02,  4.4887e-02],\n",
      "          [-5.6982e-02, -9.5813e-03,  5.0769e-02, -5.6389e-02, -3.3896e-02],\n",
      "          [ 1.3685e-02, -7.9691e-02,  3.7104e-02,  6.8019e-02,  1.1530e-02],\n",
      "          [ 1.0191e-02, -2.0623e-02,  3.0267e-02,  4.7749e-02,  6.9919e-02],\n",
      "          [ 4.0789e-02,  2.0198e-02,  2.7539e-02,  5.3723e-02,  5.4652e-02]],\n",
      "\n",
      "         [[ 8.0927e-02,  7.2905e-02,  7.6617e-02,  2.5289e-03, -6.1865e-02],\n",
      "          [ 1.6034e-02, -4.2075e-02,  7.2732e-02, -7.7846e-02, -2.3806e-02],\n",
      "          [-1.6892e-02, -5.5950e-02, -3.3125e-02,  1.1214e-02, -6.9408e-02],\n",
      "          [ 6.0252e-02, -6.0761e-02,  7.7881e-02,  1.1245e-02,  4.3268e-02],\n",
      "          [-2.0783e-02,  4.0666e-02, -5.7764e-02,  5.8669e-02,  1.6447e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0639e-03, -6.7071e-04, -6.2836e-02,  6.4382e-02,  7.3048e-02],\n",
      "          [-6.9251e-02,  3.3711e-02, -7.1530e-02,  3.3942e-02, -3.5293e-02],\n",
      "          [-1.0403e-02, -7.2183e-02, -7.0804e-02,  3.4722e-02, -2.1873e-02],\n",
      "          [ 2.3229e-02,  3.0102e-02, -1.5437e-02,  3.2994e-02,  7.7866e-02],\n",
      "          [ 7.1056e-02, -4.4441e-02, -2.1268e-02,  3.3199e-02,  4.2524e-02]],\n",
      "\n",
      "         [[-1.9713e-02,  5.7653e-03, -2.3975e-03,  7.4443e-02, -7.3457e-02],\n",
      "          [-1.9118e-02, -2.6761e-02,  2.3348e-02, -8.6099e-03,  6.0794e-02],\n",
      "          [ 2.7297e-02,  5.0068e-02,  4.5584e-02, -7.7003e-02,  3.4808e-04],\n",
      "          [-2.8258e-02, -3.6444e-03,  3.0018e-02,  5.8604e-02,  1.5064e-02],\n",
      "          [ 6.5275e-02, -6.8307e-02,  1.8698e-02, -3.1106e-05, -4.0328e-02]],\n",
      "\n",
      "         [[-9.3777e-03,  1.8238e-02, -7.6427e-02,  6.5970e-02,  5.3684e-02],\n",
      "          [ 8.8430e-03,  4.4876e-02,  3.0051e-02,  1.8981e-02,  5.7148e-02],\n",
      "          [ 5.7240e-02, -5.9747e-02, -5.9883e-02, -3.8606e-02,  1.8106e-02],\n",
      "          [ 7.5136e-02,  4.4792e-02, -7.7295e-02, -5.6242e-02,  4.2780e-02],\n",
      "          [ 7.7508e-04,  5.2419e-02,  5.5213e-03, -3.7358e-02, -5.3187e-02]],\n",
      "\n",
      "         [[-5.4656e-02, -1.9795e-02,  2.9877e-02, -5.2779e-02, -3.4263e-02],\n",
      "          [ 4.8454e-02,  7.8835e-02, -4.5378e-02,  1.1735e-02,  2.7517e-02],\n",
      "          [-6.5468e-02, -6.3963e-02,  1.8986e-02,  2.7356e-02,  5.1840e-02],\n",
      "          [-2.6075e-02, -1.1324e-02,  8.0035e-02,  1.0620e-02, -8.1535e-02],\n",
      "          [-6.2884e-02, -2.1823e-02,  1.2998e-02, -1.7486e-02,  8.0622e-03]],\n",
      "\n",
      "         [[-5.3340e-02, -3.2803e-02,  2.5025e-02, -5.8129e-02, -9.8852e-04],\n",
      "          [-8.3690e-03, -8.1397e-02, -4.4696e-02, -5.4476e-02, -5.7212e-02],\n",
      "          [ 1.3364e-02,  5.0536e-02,  1.2241e-02,  5.6216e-02, -5.9902e-02],\n",
      "          [-4.0448e-02,  4.0096e-03, -5.1175e-02, -4.7317e-02, -8.0326e-02],\n",
      "          [ 7.4575e-02, -3.4511e-02,  1.6805e-02,  3.8785e-02, -1.2942e-02]],\n",
      "\n",
      "         [[-5.5565e-02, -5.5078e-02, -4.0344e-02, -1.4913e-02, -4.5915e-02],\n",
      "          [ 3.9072e-02,  5.9282e-03,  5.0809e-02,  7.1987e-02, -1.8864e-02],\n",
      "          [ 2.6236e-02, -4.7860e-02, -2.0892e-02, -2.8060e-02, -6.5388e-02],\n",
      "          [ 3.9312e-02, -7.9431e-03,  1.0716e-02, -6.2569e-02,  4.7965e-02],\n",
      "          [ 4.8823e-02,  5.7735e-02,  3.3286e-02, -4.8090e-02,  6.6890e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8052e-02, -7.8250e-02,  4.2752e-03, -5.9744e-02,  5.1699e-03],\n",
      "          [ 7.5757e-02, -4.8296e-02, -9.3798e-03,  1.4032e-02,  3.0681e-02],\n",
      "          [-6.8501e-02, -5.4276e-02,  7.4331e-02,  3.9179e-02,  2.2001e-02],\n",
      "          [ 8.6527e-03, -4.0756e-02, -4.4505e-02, -2.2207e-02, -7.0805e-02],\n",
      "          [-1.2498e-02, -1.5747e-02,  7.7891e-02, -5.1465e-02,  6.9682e-03]],\n",
      "\n",
      "         [[ 5.6328e-02,  4.2528e-02, -6.2872e-02, -2.9708e-02, -6.1021e-02],\n",
      "          [ 6.8570e-02,  5.8868e-02, -6.9405e-02, -5.3468e-02,  1.4765e-02],\n",
      "          [-2.0590e-02,  5.9229e-02,  5.5984e-05,  9.9206e-03,  5.2986e-02],\n",
      "          [-1.9636e-02,  2.5551e-02, -7.5211e-02,  7.5058e-02,  2.0367e-02],\n",
      "          [ 3.5254e-03,  3.2206e-02,  2.5844e-02, -8.7001e-03, -2.9376e-03]],\n",
      "\n",
      "         [[-1.3969e-02, -1.1682e-02, -5.4881e-02,  2.0169e-02, -1.8594e-02],\n",
      "          [-2.5151e-04,  3.3361e-02,  5.2496e-02, -7.9318e-02,  5.7825e-02],\n",
      "          [ 5.9239e-02, -5.5873e-02, -1.8802e-02,  6.3489e-02, -6.3455e-02],\n",
      "          [ 1.7205e-03, -7.4290e-02, -8.1534e-02, -7.6479e-02, -8.8854e-03],\n",
      "          [-7.6680e-02,  2.9470e-02, -4.4129e-02,  3.0087e-02,  9.8967e-03]],\n",
      "\n",
      "         [[-6.7449e-02,  5.0473e-02,  4.7877e-02, -3.2090e-02,  3.4478e-02],\n",
      "          [-1.9848e-02,  6.6652e-02, -7.4160e-02,  2.2451e-03,  5.0009e-02],\n",
      "          [ 6.1651e-02,  4.9266e-02,  4.7526e-02, -4.8556e-02, -2.4580e-02],\n",
      "          [ 3.6625e-02,  4.3591e-02, -2.7230e-02,  2.3395e-02, -5.6400e-02],\n",
      "          [-1.4014e-02, -1.3992e-02, -6.2549e-02, -7.0844e-02, -4.4099e-02]],\n",
      "\n",
      "         [[-5.0178e-02,  1.9027e-02, -8.0061e-02,  6.9512e-02,  3.1157e-02],\n",
      "          [-1.6337e-02, -2.5524e-02, -5.6342e-02,  5.5542e-02,  3.9003e-03],\n",
      "          [-3.6808e-03,  4.4834e-02,  6.2584e-02,  6.7752e-02, -3.5098e-03],\n",
      "          [ 3.6779e-02,  5.6843e-03,  4.0554e-02,  5.4395e-02,  1.2172e-02],\n",
      "          [ 6.4181e-02,  5.2083e-02,  1.6533e-02,  6.0538e-02, -1.3564e-02]],\n",
      "\n",
      "         [[ 6.1586e-02, -2.3108e-02,  2.3489e-02,  5.8102e-02,  6.8425e-02],\n",
      "          [ 7.3447e-02, -5.0767e-02,  7.0407e-02,  2.3413e-03, -7.8470e-02],\n",
      "          [-7.8186e-02,  6.5091e-02,  4.7198e-02, -8.1299e-03,  3.8542e-02],\n",
      "          [-7.5119e-02,  6.6859e-02, -2.6676e-02, -5.3090e-02, -6.1609e-02],\n",
      "          [ 5.2082e-02,  7.0956e-02,  1.1255e-02, -4.7324e-02, -7.1134e-02]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0254, -0.0174,  0.0680,  0.0326, -0.0094,  0.0240, -0.0132,  0.0791,\n",
      "        -0.0152,  0.0715, -0.0400, -0.0002,  0.0774,  0.0336, -0.0377,  0.0537],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0168, -0.0064,  0.0417,  ...,  0.0001,  0.0028,  0.0042],\n",
      "        [-0.0341, -0.0007,  0.0494,  ...,  0.0038, -0.0354,  0.0410],\n",
      "        [-0.0290, -0.0408, -0.0087,  ..., -0.0262,  0.0436,  0.0476],\n",
      "        ...,\n",
      "        [ 0.0357,  0.0342, -0.0156,  ..., -0.0146, -0.0461,  0.0395],\n",
      "        [ 0.0219,  0.0494, -0.0340,  ...,  0.0370,  0.0107,  0.0179],\n",
      "        [-0.0026,  0.0197, -0.0209,  ...,  0.0282, -0.0196,  0.0108]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0045,  0.0317,  0.0083, -0.0386, -0.0106,  0.0350,  0.0060,  0.0015,\n",
      "         0.0367,  0.0206,  0.0223, -0.0356,  0.0084,  0.0057, -0.0448,  0.0341,\n",
      "         0.0009,  0.0186, -0.0175, -0.0360, -0.0323, -0.0212,  0.0465, -0.0300,\n",
      "         0.0007, -0.0269, -0.0157,  0.0465, -0.0318,  0.0208,  0.0144,  0.0008,\n",
      "        -0.0344,  0.0335,  0.0170,  0.0355, -0.0297,  0.0439,  0.0046, -0.0167,\n",
      "         0.0384,  0.0260,  0.0366,  0.0081, -0.0213,  0.0289,  0.0050, -0.0292,\n",
      "        -0.0104, -0.0188, -0.0186,  0.0244, -0.0355,  0.0355,  0.0480, -0.0319,\n",
      "         0.0158,  0.0083,  0.0395, -0.0056,  0.0378,  0.0440,  0.0165,  0.0385,\n",
      "         0.0192,  0.0101, -0.0092, -0.0090,  0.0278,  0.0265,  0.0182, -0.0059,\n",
      "         0.0440, -0.0498,  0.0443, -0.0034,  0.0288,  0.0092,  0.0427,  0.0349,\n",
      "        -0.0445,  0.0312,  0.0073, -0.0416,  0.0370,  0.0060, -0.0459,  0.0291,\n",
      "        -0.0086,  0.0462, -0.0118,  0.0230, -0.0442, -0.0097, -0.0185, -0.0075,\n",
      "         0.0308,  0.0191,  0.0215,  0.0437,  0.0215, -0.0499,  0.0237,  0.0135,\n",
      "         0.0023, -0.0028,  0.0031, -0.0099,  0.0192, -0.0120,  0.0409,  0.0160,\n",
      "         0.0106,  0.0025,  0.0481,  0.0455,  0.0271, -0.0388, -0.0212, -0.0083],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0021,  0.0049, -0.0800,  ...,  0.0371, -0.0872, -0.0165],\n",
      "        [ 0.0810,  0.0891,  0.0416,  ...,  0.0630, -0.0363,  0.0528],\n",
      "        [ 0.0244,  0.0837, -0.0292,  ...,  0.0874,  0.0779, -0.0104],\n",
      "        ...,\n",
      "        [ 0.0279,  0.0690,  0.0908,  ...,  0.0022,  0.0773, -0.0003],\n",
      "        [ 0.0645,  0.0622,  0.0623,  ..., -0.0294,  0.0583, -0.0041],\n",
      "        [-0.0628, -0.0517, -0.0464,  ..., -0.0056, -0.0255, -0.0411]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 8.1223e-02, -5.9995e-02,  3.4146e-02,  5.3922e-02,  2.8291e-02,\n",
      "         1.9918e-03,  7.1516e-02,  7.2051e-02,  9.0823e-03, -3.0299e-04,\n",
      "        -3.4712e-02, -8.0333e-02,  8.3877e-02,  2.1447e-02, -4.8398e-02,\n",
      "         8.7610e-02, -3.4719e-02,  1.7244e-02,  3.1204e-03, -7.7888e-03,\n",
      "        -3.3452e-02, -1.4942e-02, -4.2704e-02, -3.9432e-02, -8.2512e-02,\n",
      "        -1.8322e-02, -3.4213e-05,  4.9445e-02, -7.1410e-02, -7.7249e-03,\n",
      "        -5.6242e-02,  2.9940e-02, -2.0336e-02,  4.7584e-02,  8.3049e-02,\n",
      "        -2.8011e-02,  8.0183e-02,  1.0647e-03,  8.1164e-04, -8.8225e-02,\n",
      "        -9.3438e-03, -5.8459e-02, -7.5729e-03,  6.5076e-02,  1.2383e-02,\n",
      "         9.0234e-02,  2.9474e-02, -8.9139e-02,  6.5555e-02, -8.8563e-02,\n",
      "        -6.7949e-05, -5.3773e-02,  2.5868e-02, -8.0639e-02,  2.9834e-02,\n",
      "        -2.8105e-02, -2.9978e-02,  2.3994e-02, -1.5656e-02, -8.1748e-03,\n",
      "         1.4816e-02,  8.9507e-02,  3.6128e-02,  7.4132e-02, -8.8700e-02,\n",
      "         4.6742e-02,  2.1954e-02, -5.1286e-02, -4.5490e-02,  2.8725e-02,\n",
      "        -9.6269e-03,  4.0646e-02,  8.2562e-03,  3.3876e-02,  7.9036e-02,\n",
      "        -8.9249e-02,  3.5200e-03,  5.3867e-02,  4.4461e-02,  2.0522e-02,\n",
      "         6.9561e-02, -2.1519e-02,  3.1592e-02, -1.5547e-03],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0170, -0.0430, -0.0025, -0.0877,  0.0244,  0.0917, -0.0942, -0.0820,\n",
      "         -0.0309, -0.1037, -0.0680, -0.0682,  0.0176, -0.0143, -0.0763, -0.0503,\n",
      "         -0.0182,  0.0963, -0.0212,  0.0600, -0.0206, -0.0144,  0.0988,  0.0448,\n",
      "          0.0880, -0.0871,  0.0102, -0.0858, -0.0890, -0.0887,  0.0207,  0.1056,\n",
      "         -0.0504, -0.0360,  0.0785, -0.0180,  0.0415, -0.0073, -0.0387, -0.0964,\n",
      "          0.0223, -0.0519, -0.0877, -0.0074, -0.0142, -0.0950, -0.0935, -0.0095,\n",
      "          0.0422, -0.0983,  0.0931,  0.0274,  0.0092,  0.0941,  0.0414,  0.0085,\n",
      "         -0.0579,  0.0451,  0.0124, -0.0234, -0.0277,  0.0234, -0.0536, -0.1024,\n",
      "          0.0916, -0.0060, -0.1015,  0.0148, -0.0328,  0.0828,  0.0214, -0.0300,\n",
      "         -0.0471,  0.0092,  0.0313,  0.0024, -0.1002, -0.0709, -0.0625,  0.0739,\n",
      "          0.0799,  0.0465, -0.0270,  0.0769],\n",
      "        [ 0.0282,  0.0203,  0.0267,  0.0500, -0.0621,  0.1003, -0.0861, -0.0266,\n",
      "          0.0583, -0.0511,  0.0134, -0.0170,  0.0959,  0.0938,  0.0933,  0.0737,\n",
      "         -0.0894,  0.0399,  0.0599, -0.0375,  0.0325,  0.1070,  0.0289,  0.1021,\n",
      "          0.0586, -0.0814,  0.0712,  0.0464, -0.0905, -0.0707,  0.0986, -0.0831,\n",
      "         -0.0402,  0.0644,  0.0297,  0.0002, -0.0880, -0.0795,  0.1003,  0.0868,\n",
      "          0.0589,  0.0599, -0.0263,  0.0509,  0.0969, -0.0369,  0.0622,  0.0757,\n",
      "         -0.0559, -0.0400, -0.0544,  0.0792, -0.0341, -0.0960,  0.0210,  0.0790,\n",
      "          0.0082,  0.0535,  0.0027,  0.0978,  0.0205,  0.0427,  0.0644,  0.0546,\n",
      "         -0.1065, -0.0643,  0.0677, -0.0624,  0.1057, -0.0602, -0.0957,  0.0468,\n",
      "          0.0070,  0.0406,  0.0845, -0.0473,  0.0801, -0.0089,  0.0269,  0.0212,\n",
      "          0.1072,  0.1024, -0.0297,  0.0338],\n",
      "        [ 0.0181, -0.0197,  0.0960,  0.0512,  0.0524,  0.0314,  0.0168,  0.0538,\n",
      "         -0.0591, -0.0224, -0.0589, -0.0773, -0.0323,  0.0028, -0.0710,  0.0153,\n",
      "          0.0640,  0.0335, -0.0030, -0.0451, -0.0977, -0.0381,  0.0965,  0.1029,\n",
      "         -0.0051,  0.0388,  0.0999,  0.0069,  0.0755,  0.0279,  0.0710, -0.0727,\n",
      "         -0.0159,  0.0589, -0.0313,  0.0706, -0.0242, -0.0193,  0.0310, -0.0596,\n",
      "          0.0366, -0.1014, -0.0552,  0.0325,  0.0524, -0.0456, -0.0682,  0.0962,\n",
      "          0.0185, -0.0440,  0.0702,  0.0620, -0.0888,  0.0902, -0.0305,  0.1058,\n",
      "         -0.0995, -0.0511,  0.0099, -0.0015, -0.0272,  0.0287, -0.0818,  0.0466,\n",
      "          0.1043, -0.0770, -0.0949,  0.0352, -0.0714,  0.0246, -0.0738, -0.1001,\n",
      "         -0.0571, -0.0040,  0.0470,  0.0532, -0.0540, -0.1003,  0.0476, -0.0837,\n",
      "          0.0597, -0.0155, -0.1028, -0.0292],\n",
      "        [-0.0175,  0.0683,  0.0288, -0.1078,  0.0324, -0.0537,  0.0811,  0.0820,\n",
      "         -0.0242, -0.1041,  0.0374,  0.0703, -0.0605,  0.0604, -0.0184, -0.0019,\n",
      "         -0.0649, -0.1004,  0.0502,  0.0500, -0.0849, -0.1054,  0.0490,  0.0352,\n",
      "         -0.0310,  0.0201, -0.0376, -0.0962,  0.0814,  0.0182,  0.0640, -0.0017,\n",
      "          0.0594, -0.0219, -0.0625,  0.0382, -0.0268, -0.0084,  0.0488,  0.0949,\n",
      "         -0.0492,  0.0317,  0.0307,  0.0308, -0.1037, -0.0132,  0.0884,  0.0552,\n",
      "          0.1033, -0.0888, -0.0712,  0.0036,  0.0540, -0.0057, -0.0887,  0.0034,\n",
      "         -0.0966,  0.0035, -0.0646,  0.0033,  0.1082,  0.0521, -0.0223,  0.0884,\n",
      "         -0.0836, -0.0760, -0.0270,  0.0549,  0.0806, -0.0901,  0.0228,  0.0215,\n",
      "          0.0016, -0.0786,  0.0111, -0.0455, -0.0299, -0.0214,  0.0668, -0.0334,\n",
      "          0.0105,  0.1072,  0.0948, -0.1039],\n",
      "        [ 0.1030,  0.0742, -0.0600,  0.0619,  0.0204, -0.0490,  0.0630, -0.0697,\n",
      "          0.0835, -0.0812,  0.0870, -0.1044,  0.0900, -0.0026, -0.0292, -0.0420,\n",
      "          0.0954,  0.0813, -0.0235, -0.0034,  0.0324,  0.0796,  0.0330, -0.0763,\n",
      "          0.1031,  0.0988,  0.0825, -0.0597,  0.0747, -0.0412, -0.0175,  0.0886,\n",
      "          0.0825, -0.0235, -0.0406, -0.0671, -0.0026, -0.0687,  0.0893, -0.0783,\n",
      "          0.0611,  0.0031,  0.0783, -0.0196, -0.0866, -0.0264, -0.0617, -0.0978,\n",
      "         -0.0939, -0.0576, -0.0778, -0.0002, -0.0290,  0.0154, -0.0454, -0.0103,\n",
      "          0.0644,  0.0287, -0.0495, -0.0138,  0.0208,  0.0572, -0.0847,  0.0272,\n",
      "          0.0057, -0.0269,  0.0171,  0.0444,  0.0821, -0.0830, -0.1039,  0.0017,\n",
      "          0.0954,  0.0555,  0.0156,  0.0785, -0.0744,  0.0482, -0.0075, -0.0261,\n",
      "          0.0477,  0.0561, -0.0798, -0.0893],\n",
      "        [ 0.0456,  0.0844, -0.0426,  0.0461,  0.0663, -0.0869, -0.0734,  0.0530,\n",
      "          0.0711, -0.0890, -0.0096, -0.0566,  0.0428,  0.0775,  0.0752,  0.0400,\n",
      "         -0.0929,  0.0827, -0.0813, -0.0740,  0.0381, -0.0417, -0.0633, -0.0531,\n",
      "          0.0470, -0.0093,  0.0787, -0.1010, -0.1007,  0.0118,  0.0851, -0.0735,\n",
      "         -0.0043,  0.0714, -0.0998, -0.0448,  0.0976, -0.0809,  0.0280, -0.0098,\n",
      "          0.0726, -0.0771, -0.0381, -0.0968, -0.0166, -0.0870, -0.0627,  0.0706,\n",
      "          0.0852,  0.0253, -0.0578,  0.0158, -0.0670,  0.0829, -0.0647,  0.0230,\n",
      "         -0.0097,  0.1079,  0.0147, -0.0174, -0.0216, -0.0868, -0.0044, -0.0107,\n",
      "         -0.0693, -0.0878, -0.0511, -0.0429, -0.0302, -0.0808,  0.0902, -0.0435,\n",
      "          0.1020,  0.0903,  0.0647, -0.0555, -0.1000,  0.0518, -0.0195, -0.0553,\n",
      "          0.0506,  0.0436,  0.0171, -0.0019],\n",
      "        [ 0.0453,  0.0178,  0.0602, -0.0173,  0.0480, -0.0941, -0.0123,  0.0244,\n",
      "         -0.0880,  0.0339,  0.0601,  0.0919, -0.0299, -0.0393, -0.0912, -0.0911,\n",
      "         -0.0604, -0.0116, -0.0252, -0.0504,  0.0346,  0.0404,  0.0637,  0.0585,\n",
      "         -0.0860,  0.0140,  0.0254,  0.0731,  0.1039, -0.0490,  0.0916, -0.1045,\n",
      "         -0.0839,  0.0962, -0.0437, -0.0144,  0.0703, -0.0056,  0.0497,  0.0240,\n",
      "          0.0456, -0.0479,  0.0105, -0.0337, -0.0941, -0.0178,  0.0142,  0.1088,\n",
      "          0.0013, -0.0187, -0.0136,  0.0803, -0.0006, -0.0072, -0.0243,  0.0610,\n",
      "          0.0431,  0.0860,  0.0969, -0.0182,  0.0918, -0.0472, -0.0112,  0.0041,\n",
      "          0.0573, -0.0951,  0.0102, -0.0681, -0.0110, -0.1029,  0.0767,  0.0852,\n",
      "          0.1086,  0.0107,  0.0547,  0.0598, -0.0641, -0.0142, -0.0776,  0.0894,\n",
      "          0.0920,  0.0654, -0.0793, -0.0680],\n",
      "        [-0.1031,  0.0142,  0.0496,  0.0777, -0.0873,  0.0384,  0.0162,  0.0928,\n",
      "          0.0690, -0.0450,  0.0185,  0.0471,  0.0631, -0.0147,  0.0280, -0.0222,\n",
      "          0.0179, -0.0601, -0.0548, -0.0722, -0.1026, -0.0945,  0.0930, -0.0778,\n",
      "         -0.0889,  0.0727,  0.0337,  0.1028, -0.0050,  0.0280,  0.1026, -0.0308,\n",
      "         -0.0350, -0.0249, -0.0343, -0.0723, -0.0704,  0.0617, -0.0573,  0.0926,\n",
      "          0.0928,  0.0480,  0.0652, -0.0664,  0.1067,  0.1037,  0.0239, -0.0854,\n",
      "         -0.0117,  0.0131, -0.0486,  0.0130, -0.0550, -0.0522,  0.0160,  0.0232,\n",
      "          0.0215, -0.1016,  0.0363,  0.0155, -0.0027,  0.0005,  0.0132,  0.0433,\n",
      "          0.0123,  0.1023, -0.0017,  0.0620,  0.0027,  0.0285, -0.0307, -0.0583,\n",
      "         -0.0515,  0.0244,  0.0734, -0.0405,  0.0385,  0.0225,  0.0145,  0.0932,\n",
      "         -0.0414,  0.0907, -0.0046,  0.0272],\n",
      "        [-0.1034,  0.0641,  0.0066,  0.0378, -0.0416,  0.0017, -0.0778, -0.0730,\n",
      "         -0.0017,  0.1026,  0.0829, -0.0515,  0.0954,  0.0796, -0.0057, -0.1050,\n",
      "          0.0071, -0.0688, -0.0624, -0.0441, -0.1077, -0.0908, -0.0166,  0.0901,\n",
      "         -0.0789, -0.0112,  0.0516, -0.0262,  0.0015, -0.0672, -0.0311,  0.0640,\n",
      "         -0.0398, -0.1047, -0.0592,  0.0428, -0.0616, -0.0846,  0.0336,  0.0545,\n",
      "          0.0599, -0.0351,  0.0374,  0.0073, -0.0915,  0.0016,  0.0799,  0.0289,\n",
      "          0.0546, -0.0349, -0.0992, -0.0005,  0.0465, -0.0687, -0.1032, -0.0169,\n",
      "          0.0103, -0.0453, -0.0063, -0.0331, -0.0617,  0.0664,  0.0403, -0.0431,\n",
      "         -0.0353, -0.0546, -0.0266,  0.0711, -0.0235, -0.0009, -0.0537, -0.0960,\n",
      "         -0.1061,  0.0672,  0.0372,  0.0853, -0.0886,  0.0080, -0.0548, -0.0584,\n",
      "          0.0825,  0.0804, -0.0810, -0.0635],\n",
      "        [-0.0975,  0.0453, -0.0274,  0.0848, -0.0063, -0.0373,  0.0994,  0.0293,\n",
      "          0.0470,  0.0364, -0.0995,  0.0757, -0.0194, -0.0809,  0.0002,  0.0372,\n",
      "          0.0415, -0.1001,  0.0772,  0.0414, -0.0081, -0.0433,  0.0802, -0.0649,\n",
      "         -0.0107,  0.0220,  0.0725,  0.0090, -0.0830,  0.0498,  0.0236,  0.0066,\n",
      "         -0.0491,  0.0668,  0.1089, -0.0456, -0.0705, -0.0058,  0.1056, -0.0768,\n",
      "         -0.0653, -0.0953, -0.1069, -0.0064,  0.0479,  0.0366,  0.0932, -0.0425,\n",
      "          0.0874,  0.0154, -0.1018, -0.0173,  0.0838,  0.0483, -0.0961, -0.0112,\n",
      "          0.0338,  0.0825,  0.0784,  0.0992, -0.1069,  0.0792,  0.0930, -0.0103,\n",
      "         -0.0826,  0.0943,  0.0734, -0.0756,  0.0059,  0.1019,  0.0649,  0.0918,\n",
      "         -0.0410,  0.0800, -0.0131,  0.0758,  0.0448,  0.0053, -0.0036, -0.0882,\n",
      "          0.0284,  0.0605,  0.0857, -0.0638]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0517,  0.0721,  0.0214,  0.0544,  0.0016,  0.0998, -0.0806, -0.1029,\n",
      "        -0.0774, -0.0794], requires_grad=True)]\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "61706\n",
      "tensor([[ 0.0485,  0.1560,  0.0246,  0.0006, -0.0165,  0.0902, -0.0947, -0.0758,\n",
      "         -0.1668, -0.0440]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# The learnable parameters of a model are returned by net.parameters()\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "print(sum(p.numel() for p in params))\n",
    "\n",
    "# The input to the forward is a tensor, and so is the output\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "* Loss function은 (output, target) 을 input으로 받아 그 차이를 return한다.\n",
    "* 직접 구현할 수도 있지만 대부분의 일반적인 loss는 대부분 nn package에 구현되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.5116)\n",
      "<MseLossBackward object at 0x7ff4e097fa58>\n",
      "<AddmmBackward object at 0x7ff4e0108b38>\n",
      "<ReluBackward object at 0x7ff4e0108ba8>\n"
     ]
    }
   ],
   "source": [
    "# For example\n",
    "output = net(input)\n",
    "target = torch.arange(1, 11).unsqueeze(0)  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "# You can follow loss in the backward direction, using it's .grad_fn attribute\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "* Back-propagation을 위해서는 여러번 언급했듯이 loss.backward()를 이용한다.\n",
    "* net.zero_grad()를 이용해 먼저 모든 parameter의 gradient buffer에 0을 대입한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0364,  0.0476, -0.0230, -0.0005,  0.0717, -0.1214])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time \n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor([[ 0.0141,  0.1376,  0.0434,  0.1120, -0.0666,  0.0737,  0.0570,\n",
      "          0.0077,  0.1031,  0.1704]], device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "net.to(device)\n",
    "input = torch.randn(1, 1, 32, 32, device=device)\n",
    "\n",
    "out = net(input)\n",
    "\n",
    "print(out, out.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and DataLoader\n",
    "* Tensorflow와 가장 크게 다른 점. \n",
    "* Tensorflow에서는 dataset과 loader의 form이 구체적이지 않았으나, PyTorch에서는 dataset과 DataLoader의 구체적인 form을 제공하고 쉽게 Batch를 만들 수 있도록 한다.\n",
    "  * torch.utils.data.Dataset: Neural Network에 사용하고자 하는 dataset에서 이미지를 뽑아주는 역할을 하는 class. 여기서 image의  pre-processing을 할 수 있다.\n",
    "  * torch.utils.data.DataLoader: Dataset을 통해 전처리된 이미지를 batch_size 개수만큼 뽑아 batch를 만들어주는 역할을 하는 class. 이미지의 순서를 섞는 등의 효과를 사용할 수도 있다.\n",
    "  * 직접 구현하는 것도 가능하지만 다음 시간에 활용하고자 한다.\n",
    "* 유명한 dataset의 경우 pytorch (torchvision.datasets)에서 기본적으로 제공한다. \n",
    "  * MNIST \n",
    "  * COCO (Captions, Detection)\n",
    "  * LSUN\n",
    "  * ImageFolder\n",
    "  * Imagenet-12\n",
    "  * CIFAR\n",
    "  * STL10\n",
    "  * SVHN\n",
    "  * PhotoTour\n",
    "  \n",
    "### Transform\n",
    "* Data augmentation을 위한 변환을 자동으로 수행해주는 함수\n",
    "* torchvision.transforms에 위치하고 있음\n",
    "  * 역시 직접 구현도 가능하다. 이도 역시 다음 시간에 활용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4     0     2     2     6     4     3     4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABPCAYAAAD7qT6JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIIQJohCFwRBCBP+H+jHj1X/sO9rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4b984e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j].item() for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
