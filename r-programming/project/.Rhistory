# Read brain data (lec3_1.R)
brain<-read.csv(file="brain2210.csv")
# set working directory
# change working directory
setwd("C:/Users/uvent/source/repos/hustarAI/r-programming/exercise")
# Read brain data (lec3_1.R)
brain<-read.csv(file="brain2210.csv")
head(brain)
dim(brain)
attach(brain)
# 1-1. histogram with no options
# hist(brain$wt)
hist(wt)
help(hist)
hist(wt, col = "lightblue")
# 1-2. histogram with color and title, legend
hist(wt, breaks = 10, col = "lightblue", main="Histogram of Brain weight" , xlab="brain weight")
# see rgb values for 657 colors, choose what you like
colors()
# select colors including "blue"
grep("blue", colors(), value=TRUE)
# 1-3. fit function (find density function)
par(mfrow=c(1,1))
d <- density(brain$wt)
plot(d)
# 1-4. histogram with same scale
# multiple plot
par(mfrow=c(2,1))
brainf<-subset(brain,brain$sex=='f')
hist(brainf$wt, breaks = 12,col = "green", xlim=c(900,1700),ylim=c(0,20),cex=0.7, main="Histogram with Normal Curve (Female)", xlab="brain weight")
brainm<-subset(brain,brain$sex=='m')
hist(brainm$wt, breaks = 12,col = "orange",xlim=c(900,1700),ylim=c(0,20), main="Histogram with Normal Curve (Male)", xlab="brain weight")
hist(brainf$wt, breaks = 12,col = "green", xlim=c(900,1700),ylim=c(0,25),cex=0.7, main="Histogram with Normal Curve (Female)", xlab="brain weight")
hist(brainm$wt, breaks = 12,col = "orange",xlim=c(900,1700),ylim=c(0,25), main="Histogram with Normal Curve (Male)", xlab="brain weight")
# 2-1 boxplot for all data
boxplot(brain$wt, col=c("coral"))
# 2. boxplot
par(mfrow=c(1,2))
# 2-1 boxplot for all data
boxplot(brain$wt, col=c("coral"))
# 2-2 boxplot by group variable (female, male)
boxplot(brain$wt~brain$sex, col = c("green", "orange"))
# 2-3 horizontal boxplot
par(mfrow=c(1,1))
boxplot(brain$wt~brain$sex, boxwex=0.5, horizontal=TRUE, col = c("grey", "red"))
# 2-4 box width boxwex (width of box)
par(mfrow=c(1,2))
boxplot(brain$wt, boxwex = 0.25, col=c("coral"),  main="Boxplot for all data")
boxplot(brain$wt, boxwex = 0.5, col=c("coral"), main="Boxplot for all data")
# 2-5 add text (n) over a boxplot
par(mfrow=c(1,2))
a<-boxplot(brain$wt~brain$sex, col = c("green", "orange"))
text(c(1:nlevels(brain$sex)), a$stats[nrow(a$stats),]+30, paste("n = ",table(brain$sex),sep=""))
# example : add text (standard deviation) over  a boxplot
brainf<-subset(brain,brain$sex=='f')
brainm<-subset(brain,brain$sex=='m')
sdout<-cbind(sd(brainf$wt),sd(brainm$wt))
b<-boxplot(brain$wt~brain$sex, col = c("green", "orange"))
text(c(1:nlevels(brain$sex)), b$stats[nrow(b$stats),]+30, cex=0.8, paste("sd = ",round(sdout, 2),sep="")  )
# use autompg data (lec3_3.R)
car<-read.csv("autompg.csv")
head(car)
attach(car)
# 3. bar plot with cyliner count (lec3_3.R)
# par(mfrow=c(1,1))
table(car$cyl)
freq_cyl<-table(cyl)
names(freq_cyl) <- c ("3cyl", "4cyl", "5cyl", "6cyl",
"8cyl")
barplot(freq_cyl, col = c("lightblue", "mistyrose", "lightcyan",
"lavender", "cornsilk"))
par(mfrow=c(1,1))
barplot(freq_cyl, col = c("lightblue", "mistyrose", "lightcyan",
"lavender", "cornsilk"))
# 4. pie chart
# You can also custom the labels:
freq_cyl<-table(cyl)
names(freq_cyl) <- c ("3cyl", "4cyl", "5cyl", "6cyl", "8cyl")
# 4-1 pie chart
pie(freq_cyl)
# 4-2 pie chart clockwise
pie(freq_cyl, labels = c("3cyl", "4cyl", "5cyl", "6cyl","8cyl"),
clockwise = TRUE)
# 4-3 pie chart of subset
# subset with cylinder (4,6,8) - refresh creating subset data lec3_2.R
car1<-subset(car, cyl==4 | cyl==6 | cyl==8)
table(car1$cyl)
freq_cyl1<-table(car1$cyl)
pie(freq_cyl1, labels = c("4cyl","6cyl","8cyl"),
clockwise = TRUE)
# install package for support vector machine
# install.packages("e1071")
library (e1071)
# set working directory
setwd("C:/Users/uvent/source/repos/hustarAI/r-programming/exercise")
# read data
iris<-read.csv("iris.csv")
attach(iris)
## classification
# 1. use all data
m1<- svm(Species ~., data = iris, kernel="linear")
summary(m1)
# read data
iris<-read.csv("iris.csv")
attach(iris)
## classification
# 1. use all data
m1<- svm(Species ~., data = iris, kernel="linear")
summary(m1)
summary(iris)
## classification
# 1. use all data
m1<- svm(Species ~., data = iris, kernel="linear")
View(iris)
iris$Species <- as.factor(iris$Species)
## classification
# 1. use all data
m1<- svm(Species ~., data = iris, kernel="linear")
summary(m1)
# classify all data using svm result (m1)
# first 4 variables as attribute variables
x<-iris[, -5]
pred <- predict(m1, x)
# Check accuracy (compare predicted class(pred) and true class(y))
# y <- Species or y<-iris[,5]
y<-iris[,5]
table(pred, y)
# visualize classes by color
plot(m1, iris,  Petal.Width~Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
# install package for confusionMatrix
# install.packages("caret")
library(caret)
# training (100) & test set (50)
set.seed(1000, sample.kind="Rounding")
N=nrow(iris)
tr.idx=sample(1:N, size=N*2/3, replace=FALSE)
# target variable
y=iris[,5]
# split train data and test data
train=iris[tr.idx,]
test=iris[-tr.idx,]
version
-v
v
m1<-svm(Species~., data = train)
summary(m1)
m2<-svm(Species~., data = train,kernel="polynomial")
summary(m2)
m3<-svm(Species~., data = train,kernel="sigmoid")
summary(m3)
m4<-svm(Species~., data = train,kernel="linear")
summary(m4)
#measure accuracy
pred11<-predict(m1,test) # radial basis
confusionMatrix(pred11, test$Species)
pred12<-predict(m2,test) # polynomial
confusionMatrix(pred12, test$Species)
pred13<-predict(m3,test) # simoid
confusionMatrix(pred13, test$Species)
pred14<-predict(m4,test) # linear
confusionMatrix(pred14, test$Species)
CrossTable(test$Species, pred14)
default.stringsAsFactors()
# read data
cancer<-read.csv("cancer.csv")
#decision tree packages download
#install.packages("tree")
#load library
library(tree)
#package for confusion matrix
#install.packages("caret")
library(caret)
# set working directory
setwd("C:/Users/uvent/source/repos/hustarAI/r-programming/exercise")
# read csv file
iris<-read.csv("iris.csv")
attach(iris)
# training (n=100)/ test data(n=50)
set.seed(1000)
N<-nrow(iris)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train<-iris[tr.idx,]
test<-iris[-tr.idx,]
# step1 : growing tree
treemod<-tree(Species~., data=train)
treemod
plot(treemod)
# step1 : growing tree
treemod<-tree(Species~., data=train)
treemod
plot(treemod)
View(treemod)
text(treemod,cex=1.5)
# training (n=100)/ test data(n=50)
set.seed(123)
N<-nrow(iris)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train<-iris[tr.idx,]
test<-iris[-tr.idx,]
help("tree")
# step1 : growing tree
treemod<-tree(Species~., data=train)
treemod
plot(treemod)
text(treemod,cex=1.5)
View(iris)
# read csv file
iris<-read.csv("iris.csv")
attach(iris)
iris$Species <- as.factor(iris$Species)
# training (n=100)/ test data(n=50)
set.seed(1000)
N<-nrow(iris)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train<-iris[tr.idx,]
test<-iris[-tr.idx,]
help("tree")
# step1 : growing tree
treemod<-tree(Species~., data=train)
treemod
plot(treemod)
text(treemod,cex=1.5)
table(train$Species)
# step2 : pruning using cross-validation
cv.trees<-cv.tree(treemod, FUN=prune.misclass)
cv.trees
plot(cv.trees)
# final tree model with the optimal node
prune.trees<-prune.misclass(treemod, best=3)
plot(prune.trees)
text(prune.trees,pretty=0, cex=1.5)
# step 3: classify test data
treepred<-predict(prune.trees,test,type='class')
# classification accuracy
confusionMatrix(treepred,test$Species)
prune.trees
treepred2<-predict(treemod, test, type='class')
confusionMatrix(treemod, test$Species)
confusionMatrix(treepred2, test$Species)
heart <- read.csv("heart.csv")
("C:/Users/uvent/source/repos/hustarAI/r-programming/project")
("C:/Users/uvent/source/repos/hustarAI/r-programming/project")
setwd("C:/Users/uvent/source/repos/hustarAI/r-programming/project")
heart <- read.csv("heart.csv")
heart$target <- as.factor(heart$target)
attach(heart)
# 1.1
set.seed(1000)
N<-nrow(heart)
tr.idx<-sample(1:N, size=2/3, replace=FALSE)
train<-heart[tr.idx,]
test<-heart[-tr.idx,]
# 1.1
set.seed(1000)
N<-nrow(heart)
tr.idx<-sample(1:N, size=2/3, replace=FALSE)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
train<-heart[tr.idx,]
test<-heart[-tr.idx,]
# 1.2
m1<-svm(target~. data = train, kernel='radial')
m2<-svm(target~. data = train, kernel='polynomial')
m3<-svm(target~. data = train, kernel='sigmoid')
m4<-svm(target~. data = train, kernel='linear')
# 1.2
m1<-svm(target~., data=train, kernel='radial')
m2<-svm(target~., data=train, kernel='polynomial')
m3<-svm(target~., data=train, kernel='sigmoid')
m4<-svm(target~., data=train, kernel='linear')
summary(m1)
summary(m2)
summary(m3)
summary(m4)
library(e1071)
library(caret)
pred1<-predict(m1, test)
pred2<-predict(m2, test)
pred3<-predict(m3, test)
pred4<-predict(m4, test)
confusionMatrix(pred1, test$target)
confusionMatrix(pred2, test$target)
confusionMatrix(pred3, test$target)
confusionMatrix(pred4, test$target)
# random forest package
install.packages("randomForest")
# random forest package
# install.packages("randomForest")
library(randomForest)
help(randomForest)
library(tree)
m5<-tree(target~., data=train)
summary(m5)
plot(m5)
text(m5, ces=0.5)
text(m5, ces=0.3)
text(m5, ces=0.5)
text(m5, cex=0.5)
plot(m5)
text(m5, cex=0.5)
plot(m5)
text(m5, cex=1)
# 1.4
model-b<-cv.tree(model-a, FUN=prune.misclass)
model1<-tree(target~., data=train)
summary(model1)
plot(model1)
text(model1, cex=1)
# 1.4
model2<-cv.tree(model1, FUN=prune.misclass)
model2
plot(model2)
treepred1<-predict(model1, test, type='class')
confusionMatrix(treepred1,test$target)
treepred2<-predict(model2, test, type='class')
confusionMatrix(treepred2,test$target)
# 1.4
model2<-cv.tree(model1, FUN=prune.misclass)
treepred2<-predict(model2, test, type='class')
model2<-prune.misclass(model1, best=6)
treepred2<-predict(model2, test, type='class')
confusionMatrix(treepred2,test$target)
setwd("C:/Users/uvent/source/repos/hustarAI/r-programming/project")
heart <- read.csv("heart.csv")
heart$target <- as.factor(heart$target)
attach(heart)
# 1.1
set.seed(1000)
N<-nrow(heart)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
train<-heart[tr.idx,]
test<-heart[-tr.idx,]
model1<-tree(target~., data=train)
summary(model1)
plot(model1)
text(model1, cex=1)
treepred1<-predict(model1, test, type='class')
confusionMatrix(treepred1,test$target)
model1
# 1.4
cv.tr<-cv.tree(model1, FUN=prune.misclass)
cv.tr
plot(cv.tr)
model2<-prune.misclass(model1, best=6)
plot(model2)
text(model2)
treepred2<-predict(model2, test, type='class')
confusionMatrix(treepred2,test$target)
plot(model1)
text(model1, cex=0.5)
library(e1071)
library(caret)
library(tree)
setwd("C:/Users/uvent/source/repos/hustarAI/r-programming/project")
heart <- read.csv("heart.csv")
heart$target <- as.factor(heart$target)
attach(heart)
# 1.1
set.seed(1000)
N<-nrow(heart)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
train<-heart[tr.idx,]
test<-heart[-tr.idx,]
# 1.2
m1<-svm(target~., data=train, kernel='radial')
m2<-svm(target~., data=train, kernel='polynomial')
m3<-svm(target~., data=train, kernel='sigmoid')
m4<-svm(target~., data=train, kernel='linear')
summary(m1)
summary(m2)
summary(m3)
summary(m4)
pred1<-predict(m1, test)
pred2<-predict(m2, test)
pred3<-predict(m3, test)
pred4<-predict(m4, test)
confusionMatrix(pred1, test$target)
confusionMatrix(pred2, test$target)
confusionMatrix(pred3, test$target)
confusionMatrix(pred4, test$target)
model1<-tree(target~., data=train)
summary(model1)
model1
plot(model1)
text(model1, cex=0.5)
treepred1<-predict(model1, test, type='class')
confusionMatrix(treepred1,test$target)
# 1.4
cv.tr<-cv.tree(model1, FUN=prune.misclass)
cv.tr
plot(cv.tr)
model2<-prune.misclass(model1, best=6)
plot(model2)
text(model2)
treepred2<-predict(model2, test, type='class')
confusionMatrix(treepred2,test$target)
